{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Isjqqa84yJv-"
   },
   "source": [
    "**stage2_DANN_HPsearch.ipynb**. This notebook attempts to search the hyperparameter that yields the optimal validation performance.\n",
    "\n",
    "**Edit**<br/>\n",
    "\n",
    "**TODO**<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nswy3ke-TUyA"
   },
   "source": [
    "# Import packages and get authenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dR0gs1Ya0xmy"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DD6EM010PDWn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/中研院/repo/')\n",
    "# sys.path.append('~/project_FDDAT/repo/')\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "from falldetect.utilities import *\n",
    "from falldetect.models import *\n",
    "from falldetect.dataset_util import *\n",
    "from falldetect.training_util import *\n",
    "from falldetect.eval_util import *\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "# Plotting\n",
    "# checklist 1: comment inline, uncomment Agg\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "\n",
    "# matplotlib.rc( 'savefig', transparent=True )\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# batch_size = 20\n",
    "# class_sample_count = [10, 1, 20, 3, 4] # dataset has 10 class-1 samples, 1 class-2 samples, etc.\n",
    "# weights = 1 / torch.Tensor(class_sample_count)\n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, batch_size)\n",
    "# trainloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, sampler = sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get user inputs\n",
    "In ipython notebook, these are hardcoded. In production python code, use parsers to provide these inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='FD_DAT')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "parser.add_argument('--extractor_type', metavar='extractor_type', help='extractor_type',\n",
    "                    default='CNN')\n",
    "parser.add_argument('--num_epochs', type=int, metavar='num_epochs', help='number of epochs',\n",
    "                    default='5')\n",
    "parser.add_argument('--CV_n', type=int, metavar='CV_n', help='CV folds',\n",
    "                    default='2')\n",
    "parser.add_argument('--rep_n', type=int, metavar='rep_n', help='number of repitition',\n",
    "                    default='5')\n",
    "# parser.add_argument('--cuda_i', type=int, metavar='cuda_i', help='cuda index',\n",
    "#                     default='1')\n",
    "parser.add_argument('--tasks_list', metavar='tasks_list', help='a list of all tasks',\n",
    "                    default='UMAFall_waist_UPFall_belt UPFall_wrist_UMAFall_ankle')\n",
    "parser.add_argument('--show_diagnosis_plt', metavar='show_diagnosis_plt', help='show diagnosis plt or not',\n",
    "                    default='False')\n",
    "\n",
    "parser.add_argument('--use_WeightedRandomSampler', metavar='use_WeightedRandomSampler', help='use WeightedRandomSampler to mitigate imbalanced dataset',\n",
    "                    default='False')\n",
    "\n",
    "\n",
    "# split_mode = 'LOO'\n",
    "# split_mode = '5fold'\n",
    "\n",
    "# checklist 2: comment first line, uncomment second line seizures_FN\n",
    "# plt.style.use(['dark_background'])\n",
    "args = parser.parse_args(['--input_folder', '../../data_mic/stage1/preprocessed_18hz_5fold', \n",
    "                          '--output_folder', '../../data_mic/stage2/test',\n",
    "#                           '--training_params_file', 'training_params_list_v1.json',\n",
    "                          '--training_params_file', 'training_params_list_fixed.json',\n",
    "                          '--extractor_type', 'CNN',\n",
    "                          '--num_epochs', '15',\n",
    "                          '--CV_n', '2',\n",
    "                          '--rep_n', '2',\n",
    "                          '--show_diagnosis_plt', 'True',\n",
    "                          '--use_WeightedRandomSampler', 'True',\n",
    "                          '--tasks_list', 'UMAFall_leg-UPFall_rightpocket UPFall_rightpocket-UMAFall_leg',])\n",
    "#                           '--tasks_list', 'FARSEEING_thigh-FARSEEING_lowback FARSEEING_lowback-FARSEEING_thigh',])\n",
    "                          \n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.show_diagnosis_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = home+'/project_FDDAT/'\n",
    "input_folder = args.input_folder\n",
    "output_folder = args.output_folder\n",
    "training_params_file = args.training_params_file\n",
    "extractor_type = args.extractor_type\n",
    "num_epochs = args.num_epochs\n",
    "CV_n = args.CV_n\n",
    "rep_n = args.rep_n\n",
    "if args.show_diagnosis_plt == 'True':\n",
    "     show_diagnosis_plt = True\n",
    "elif args.show_diagnosis_plt == 'False':\n",
    "     show_diagnosis_plt = False\n",
    "\n",
    "if args.use_WeightedRandomSampler == 'True':\n",
    "     use_WeightedRandomSampler = True\n",
    "elif args.use_WeightedRandomSampler == 'False':\n",
    "     use_WeightedRandomSampler = False\n",
    "        \n",
    "with open('../../repo/falldetect/params.json') as json_file:\n",
    "    falldetect_params = json.load(json_file)\n",
    "\n",
    "cuda_i = falldetect_params['cuda_i']\n",
    "\n",
    "tasks_list = []\n",
    "for item in args.tasks_list.split(' '):\n",
    "    tasks_list.append((item.split('-')[0], item.split('-')[1]))\n",
    "    \n",
    "inputdir = input_folder+'/'\n",
    "outputdir = output_folder+'/'\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "    \n",
    "# test_mode = 'test' in outputdir.split('/')[-2]\n",
    "# test_mode = 'test' in training_params_file\n",
    "\n",
    "device = torch.device('cuda:{}'.format(int(cuda_i)) if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RdfojPmJEB-"
   },
   "source": [
    "# new arch HP search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params_list = [\n",
    "#   {\n",
    "#     'HP_name': 'HP_i0',\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': CV_n,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'channel_n': 4,\n",
    "#     'batch_size': 16,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'extractor_type': extractor_type,\n",
    "# #     'device': device,\n",
    "#     'dropout': 0.5,\n",
    "#     'hiddenDim_f': 3,\n",
    "#     'hiddenDim_y': 3,\n",
    "#     'hiddenDim_d': 3,\n",
    "#     'win_size': 18,\n",
    "#     'win_stride': 6,\n",
    "#     'step_n': 9,\n",
    "#     'show_diagnosis_plt': show_diagnosis_plt,\n",
    "#   },\n",
    "\n",
    "#   {\n",
    "#     'HP_name': 'HP_i1',\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': CV_n,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'channel_n': 4,\n",
    "#     'batch_size': 4,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'extractor_type': extractor_type,\n",
    "# #     'device': device,\n",
    "#     'dropout': 0.5,\n",
    "#     'hiddenDim_f': 3,\n",
    "#     'hiddenDim_y': 3,\n",
    "#     'hiddenDim_d': 3,\n",
    "#     'win_size': 18,\n",
    "#     'win_stride': 6,\n",
    "#     'step_n': 9,\n",
    "#     'show_diagnosis_plt': show_diagnosis_plt,\n",
    "#   },\n",
    "\n",
    "#   {\n",
    "#     'HP_name': 'HP_i2',\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': CV_n,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'channel_n': 4,\n",
    "#     'batch_size': 64,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'extractor_type': extractor_type,\n",
    "# #     'device': device,\n",
    "#     'dropout': 0.5,\n",
    "#     'hiddenDim_f': 3,\n",
    "#     'hiddenDim_y': 3,\n",
    "#     'hiddenDim_d': 3,\n",
    "#     'win_size': 18,\n",
    "#     'win_stride': 6,\n",
    "#     'step_n': 9,\n",
    "#     'show_diagnosis_plt': show_diagnosis_plt,\n",
    "#   },\n",
    "    \n",
    "#   {\n",
    "#     'HP_name': 'HP_i3',\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': CV_n,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'channel_n': 16,\n",
    "#     'batch_size': 4,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'extractor_type': extractor_type,\n",
    "# #     'device': device,\n",
    "#     'dropout': 0.5,\n",
    "#     'hiddenDim_f': 3,\n",
    "#     'hiddenDim_y': 3,\n",
    "\n",
    "#     'hiddenDim_d': 3,\n",
    "#     'win_size': 18,\n",
    "#     'win_stride': 6,\n",
    "#     'step_n': 9,\n",
    "#     'show_diagnosis_plt': show_diagnosis_plt,\n",
    "#   },\n",
    "\n",
    "#   {\n",
    "#     'HP_name': 'HP_i4',\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': CV_n,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'channel_n': 32,\n",
    "#     'batch_size': 4,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'extractor_type': extractor_type,\n",
    "# #     'device': device,\n",
    "#     'dropout': 0.5,\n",
    "#     'hiddenDim_f': 3,\n",
    "#     'hiddenDim_y': 3,\n",
    "#     'hiddenDim_d': 3,\n",
    "#     'win_size': 18,\n",
    "#     'win_stride': 6,\n",
    "#     'step_n': 9,\n",
    "#     'show_diagnosis_plt': show_diagnosis_plt,\n",
    "#   },\n",
    "\n",
    "#   {\n",
    "#     'HP_name': 'HP_i5',\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': CV_n,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'channel_n': 4,\n",
    "#     'batch_size': 4,\n",
    "#     'learning_rate': 0.001,\n",
    "#     'extractor_type': extractor_type,\n",
    "# #     'device': device,\n",
    "#     'dropout': 0.5,\n",
    "#     'hiddenDim_f': 3,\n",
    "#     'hiddenDim_y': 3,\n",
    "#     'hiddenDim_d': 3,\n",
    "#     'win_size': 18,\n",
    "#     'win_stride': 6,\n",
    "#     'step_n': 9,\n",
    "#     'show_diagnosis_plt': show_diagnosis_plt,\n",
    "#   },\n",
    "\n",
    "#   {\n",
    "#     'HP_name': 'HP_i6',\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': CV_n,\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'channel_n': 4,\n",
    "#     'batch_size': 4,\n",
    "#     'learning_rate': 0.0001,\n",
    "#     'extractor_type': extractor_type,\n",
    "# #     'device': device,\n",
    "#     'dropout': 0.5,\n",
    "#     'hiddenDim_f': 3,\n",
    "#     'hiddenDim_y': 3,\n",
    "#     'hiddenDim_d': 3,\n",
    "#     'win_size': 18,\n",
    "#     'win_stride': 6,\n",
    "#     'step_n': 9,\n",
    "#     'show_diagnosis_plt': show_diagnosis_plt,\n",
    "#   }, ]\n",
    "\n",
    "# with open('training_params_list.json', 'w') as fout:\n",
    "#     json.dump(training_params_list, fout, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "    \n",
    "for training_params in training_params_list:\n",
    "    training_params['CV_n'] = CV_n\n",
    "    training_params['rep_n'] = rep_n\n",
    "#     training_params['CV_list'] = CV_list\n",
    "    training_params['num_epochs'] = num_epochs\n",
    "    training_params['extractor_type'] = extractor_type\n",
    "    training_params['device'] = device\n",
    "    training_params['show_diagnosis_plt'] = show_diagnosis_plt\n",
    "    training_params['use_WeightedRandomSampler'] = use_WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rep(df_metric_keys, tgt_name, training_params, inputdir, task_outputdir, rep_n=5):\n",
    "    # 5. run rep experiments\n",
    "    df_sample = pd.DataFrame('', index=['channel_n', 'batch_size', 'learning_rate', \n",
    "                                        'source', 'DANN', 'target', 'domain', 'time_elapsed', 'num_params', 'PAD_source', 'PAD_DANN'], columns=[])\n",
    "    df_dict_agg_rep = dict( zip(df_metric_keys,[df_sample.copy(), df_sample.copy(), df_sample.copy(), df_sample.copy()]))\n",
    "\n",
    "    for i_rep in range(rep_n):\n",
    "        df_dict = performance_table(src_name, tgt_name, training_params, i_rep, inputdir, task_outputdir+training_params['HP_name']+'/')\n",
    "\n",
    "        for df_name in df_dict_agg_rep.keys():\n",
    "            df_dict_agg_rep[df_name]['rep{}'.format(i_rep)] = df_dict[df_name].copy()\n",
    "\n",
    "    df_outputdir = task_outputdir+training_params['HP_name']+'/'\n",
    "#     task_outputdir+'repetitive_results/'\n",
    "    if not os.path.exists(df_outputdir):\n",
    "        os.makedirs(df_outputdir)\n",
    "    print('df_dict_agg_rep saved at', df_outputdir)\n",
    "\n",
    "    for df_name in df_dict_agg_rep.keys():\n",
    "        df_dict_agg_rep[df_name] = get_rep_stats(df_dict_agg_rep[df_name], rep_n)\n",
    "        df_dict_agg_rep[df_name].to_csv(df_outputdir+'df_performance_table_agg_rep_{}.csv'.format(df_name.split('_')[1]), encoding='utf-8')\n",
    "\n",
    "    return df_dict_agg_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rc2TiHRJIOu"
   },
   "outputs": [],
   "source": [
    "# fine-tuning\n",
    "df_metric_keys = ['df_acc', 'df_sensitivity', 'df_precision', 'df_F1']\n",
    "\n",
    "for task_item in tasks_list:\n",
    "    (src_name, tgt_name) = task_item\n",
    "\n",
    "    task_outputdir = '{}{}_{}/'.format(outputdir, src_name, tgt_name)\n",
    "    if not os.path.exists(task_outputdir):\n",
    "        os.makedirs(task_outputdir)\n",
    "    print('outputdir for stage2 {} output: {}'.format(task_item, task_outputdir))\n",
    "\n",
    "    df_sample = pd.DataFrame('', index=['channel_n', 'batch_size', 'learning_rate', \n",
    "                                              'source', 'DANN', 'target', 'domain', 'time_elapsed', 'num_params', 'PAD_source', 'PAD_DANN'], columns=[])\n",
    "    df_dict_agg_HP = dict( zip(df_metric_keys,[df_sample.copy(), df_sample.copy(), df_sample.copy(), df_sample.copy()]))\n",
    "\n",
    "    # 1. try all HP\n",
    "    for i, training_params in enumerate(training_params_list):\n",
    "        df_dict_agg_rep = run_rep(df_metric_keys, tgt_name, training_params, inputdir, task_outputdir, rep_n)\n",
    "        training_params['df_dict_agg_rep'] = copy.deepcopy(df_dict_agg_rep)\n",
    "\n",
    "        for df_name in df_dict_agg_rep.keys():\n",
    "            print('show', df_name)\n",
    "            display(df_dict_agg_rep[df_name])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rep_stats2(df_performance_table_agg, rep_n):\n",
    "# \tdf_acc = df_performance_table_agg.loc[ ['source', 'DANN', 'target', 'domain', 'PAD_source', 'PAD_DANN'] , ].copy()\n",
    "# \tdf_params = df_performance_table_agg.loc[ ['channel_n', 'batch_size', 'learning_rate', 'time_elapsed', 'num_params'], ].copy()\n",
    "\n",
    "#     # accs\n",
    "# \tdf_performance_table_all_mean = df_acc.applymap(get_mean)\n",
    "# \tdf_performance_table_means = df_performance_table_all_mean.mean(axis=1)\n",
    "# \tdf_performance_table_stds = df_performance_table_all_mean.std(axis=1)\n",
    "# \tdf_performance_table_all_mean['mean'] = df_performance_table_means\n",
    "# \tdf_performance_table_all_mean['std'] = df_performance_table_stds\n",
    "# \tdf_performance_table_all_mean['rep'] = df_performance_table_all_mean[['mean', 'std']].apply(lambda x : '{:.3f}±{:.3f}'.format(x[0],x[1]), axis=1)\n",
    "\n",
    "#     # params\n",
    "# \tdf_params_means = df_params.mean(axis=1)\n",
    "\n",
    "# \tdf_performance_table_agg['rep_avg'] = ''\n",
    "# \tdf_performance_table_agg.loc[ ['source','DANN','target','domain','PAD_source','PAD_DANN'] , ['rep_avg']] = df_performance_table_all_mean.loc[:, 'rep']\n",
    "# \tdf_performance_table_agg.loc[ ['channel_n','batch_size','learning_rate','time_elapsed','num_params'], ['rep_avg']] = df_params_means\n",
    "# \treturn df_performance_table_agg\n",
    "\n",
    "# get_rep_stats2(df_dict_agg_rep[df_name], rep_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision=np.asarray(range(100))/100\n",
    "# sensitivity=np.asarray(range(100))/100\n",
    "# F1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
    "# F1\n",
    "# # plt.plot(np.asarray(range(100))/100, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUHTmGSnPEG+rEpBubjphB",
   "collapsed_sections": [],
   "name": "stage2_DANN_HPsearch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (FD_DAT)",
   "language": "python",
   "name": "fd_dat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
