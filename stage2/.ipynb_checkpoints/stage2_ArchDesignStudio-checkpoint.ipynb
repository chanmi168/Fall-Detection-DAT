{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nswy3ke-TUyA"
   },
   "source": [
    "# Import packages and get authenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27374,
     "status": "ok",
     "timestamp": 1586171966921,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "dR0gs1Ya0xmy",
    "outputId": "1c4a4193-695c-4447-ed1f-a6389500d78b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DD6EM010PDWn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/中研院/repo/')\n",
    "# sys.path.append('~/project_FDDAT/repo/')\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "from falldetect.utilities import *\n",
    "from falldetect.models import *\n",
    "from falldetect.dataset_util import *\n",
    "from falldetect.training_util import *\n",
    "from falldetect.eval_util import *\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# Plotting\n",
    "# checklist 1: comment inline, uncomment Agg\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwW5pmvqVMhg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get user inputs\n",
    "In ipython notebook, these are hardcoded. In production python code, use parsers to provide these inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='FD_DAT')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--extractor_type', metavar='extractor_type', help='extractor_type',\n",
    "                    default='CNN')\n",
    "parser.add_argument('--num_epochs', type=int, metavar='num_epochs', help='number of epochs',\n",
    "                    default='5')\n",
    "parser.add_argument('--CV_n', type=int, metavar='CV_n', help='CV folds',\n",
    "                    default='2')\n",
    "parser.add_argument('--rep_n', type=int, metavar='rep_n', help='number of repitition',\n",
    "                    default='5')\n",
    "parser.add_argument('--cuda_i', type=int, metavar='cuda_i', help='cuda index',\n",
    "                    default='1')\n",
    "parser.add_argument('--tasks_list', metavar='tasks_list', help='a list of all tasks',\n",
    "                    default='UMAFall_waist_UPFall_belt UPFall_wrist_UMAFall_ankle')\n",
    "parser.add_argument('--show_diagnosis_plt', metavar='show_diagnosis_plt', help='show diagnosis plt or not',\n",
    "                    default='False')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split_mode = 'LOO'\n",
    "# split_mode = '5fold'\n",
    "\n",
    "# checklist 2: comment first line, uncomment second line seizures_FN\n",
    "args = parser.parse_args(['--input_folder', '../../data_mic/stage1_preprocessed_NormalforAllAxes_18hz_5fold', \n",
    "                          '--output_folder', '../../data_mic/stage2_modeloutput_NormalforAllAxes_18hz_5fold',\n",
    "                          '--extractor_type', 'CNN',\n",
    "                          '--num_epochs', '10',\n",
    "                          '--CV_n', '2',\n",
    "                          '--rep_n', '2',\n",
    "                          '--cuda_i', '2',\n",
    "                          '--show_diagnosis_plt', 'True',\n",
    "                          '--tasks_list', 'UPFall_wrist-UMAFall_wrist',])\n",
    "#                           '--tasks_list', 'UMAFall_waist-UMAFall_wrist UPFall_wrist-UMAFall_ankle',])\n",
    "                          \n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = home+'/project_FDDAT/'\n",
    "input_folder = args.input_folder\n",
    "output_folder = args.output_folder\n",
    "extractor_type = args.extractor_type\n",
    "num_epochs = args.num_epochs\n",
    "CV_n = args.CV_n\n",
    "rep_n = args.rep_n\n",
    "show_diagnosis_plt = bool(args.show_diagnosis_plt)\n",
    "\n",
    "with open('../../repo/falldetect/params.json') as json_file:\n",
    "    falldetect_params = json.load(json_file)\n",
    "\n",
    "cuda_i = falldetect_params['cuda_i']\n",
    "\n",
    "tasks_list = []\n",
    "for item in args.tasks_list.split(' '):\n",
    "    tasks_list.append((item.split('-')[0], item.split('-')[1]))\n",
    "    \n",
    "inputdir = input_folder+'/'\n",
    "outputdir = output_folder+'/'\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "    \n",
    "test_mode = 'test' in outputdir.split('/')[-2]\n",
    "\n",
    "device = torch.device('cuda:{}'.format(int(cuda_i)) if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'HP_name': 'hp',\n",
    "    'classes_n': 2,\n",
    "    'CV_n': CV_n,\n",
    "    'num_epochs': num_epochs,\n",
    "    'channel_n': 4,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 0.0001,\n",
    "    'extractor_type': extractor_type,\n",
    "    'device': device,\n",
    "    'dropout': 0.5,\n",
    "    'hiddenDim_f': 3,\n",
    "    'hiddenDim_y': 3,\n",
    "    'hiddenDim_d': 3,\n",
    "    'win_size': 18,\n",
    "    'win_stride': 6,\n",
    "    'step_n': 9,\n",
    "    'show_diagnosis_plt': show_diagnosis_plt,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self, input_dim=50, channel_n=16):\n",
    "      super(FeatureExtractor, self).__init__()\n",
    "      self.layer1 = nn.Sequential(\n",
    "          nn.Conv1d(3, channel_n, kernel_size=3, stride=1, padding=2),\n",
    "          nn.BatchNorm1d(channel_n),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "      self.layer2 = nn.Sequential(\n",
    "          nn.Conv1d(channel_n, channel_n*2, kernel_size=3, stride=1, padding=2),\n",
    "          nn.BatchNorm1d(channel_n*2),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "      \n",
    "      cnn_layer1_dim = (input_dim+2*2-1*(3-1)-1)+1\n",
    "      pool_layer1_dim = (cnn_layer1_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "      cnn_layer2_dim = (pool_layer1_dim+2*2-1*(3-1)-1)+1\n",
    "      pool_layer2_dim = (cnn_layer2_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "      self.feature_out_dim = pool_layer2_dim*channel_n*2\n",
    "      pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "      print('FeatureExtractor_total_params:', pytorch_total_params)\n",
    "      \n",
    "  def forward(self, x):\n",
    "    out1 = self.layer1(x.float())\n",
    "    out2 = self.layer2(out1)\n",
    "    out2 = out2.reshape(out2.size(0), -1)\n",
    "    return out2\n",
    "\n",
    "# fall classifier neural network (fc layers)\n",
    "class ClassClassifier(nn.Module):\n",
    "  def __init__(self, num_classes=10, input_dim=50):\n",
    "      super(ClassClassifier, self).__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, num_classes)\n",
    "      self.relu = nn.ReLU()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out1 = self.relu(self.fc1(x.float()))\n",
    "    out2 = self.fc2(out1)\n",
    "    return out2\n",
    "\n",
    "# domain classifier neural network (fc layers)\n",
    "class DomainClassifier(nn.Module):\n",
    "  def __init__(self, num_classes=10, input_dim=50):\n",
    "      super(DomainClassifier, self).__init__()\n",
    "#       self.fc = nn.Linear(input_dim, num_classes)\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, num_classes)\n",
    "      self.relu = nn.ReLU()\n",
    "      \n",
    "  def forward(self, x, constant):\n",
    "    out1 = GradReverse.grad_reverse(x.float(), constant)\n",
    "    out1 = self.relu(self.fc1(out1))\n",
    "    out2 = self.fc2(out1)\n",
    "    return out2\n",
    "    # out2 = F.relu(self.fc(out1))\n",
    "#     out2 = self.fc(out1)\n",
    "#     return out2\n",
    "\n",
    "\n",
    "class DannModel2(nn.Module):\n",
    "  def __init__(self, device, class_N=2, domain_N=2, channel_n=16, input_dim=10):\n",
    "    super(DannModel, self).__init__()\n",
    "    self.feature_extractor = FeatureExtractor(input_dim=input_dim, channel_n=channel_n).to(device).float()\n",
    "    cnn_layer1_dim = (input_dim+2*2-1*(3-1)-1)+1\n",
    "    pool_layer1_dim = (cnn_layer1_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "    cnn_layer2_dim = (pool_layer1_dim+2*2-1*(3-1)-1)+1\n",
    "    pool_layer2_dim = (cnn_layer2_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "    feature_out_dim = int(pool_layer2_dim*channel_n*2)\n",
    "    self.class_classfier = ClassClassifier(num_classes=class_N, input_dim=feature_out_dim).to(device).float()\n",
    "    self.domain_classifier = DomainClassifier(num_classes=domain_N, input_dim=feature_out_dim).to(device).float()\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    print('DannModel_total_params:', pytorch_total_params)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    feature_out = self.feature_extractor(x)\n",
    "    class_output = self.class_classfier(feature_out)\n",
    "    domain_output = self.domain_classifier(feature_out, 1)\n",
    "    return feature_out, class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks_list[0]\n",
    "# src_name = 'UMAFall_wrist'\n",
    "# tgt_name = 'UPFall_wrist'\n",
    "\n",
    "\n",
    "# src_name = 'UPFall_wrist'\n",
    "# tgt_name = 'UMAFall_wrist'\n",
    "i_CV = 1\n",
    "\n",
    "src_names = ['UPFall_neck','UPFall_wrist','UPFall_belt','UPFall_rightpocket','UPFall_ankle',\n",
    "             'UMAFall_chest','UMAFall_wrist','UMAFall_waist','UMAFall_leg','UMAFall_ankle',\n",
    "             'SFDLA_chest','SFDLA_wrist','SFDLA_waist','SFDLA_thigh','SFDLA_ankle',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch(train_loader, val_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    data = src_train_loader.dataset.data.to(device)\n",
    "    labels = src_train_loader.dataset.labels.to(device).long()\n",
    "    feature_out, class_out, _ = model(data)\n",
    "    out_sigmoid = torch.sigmoid(class_out).data.detach().cpu().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5), dpi=80)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.plot(out_sigmoid[:,1],'.b', label='src_class_sigmoid', markersize=3)\n",
    "    ax1.plot(out_sigmoid[:,1].round(),'b', alpha=0.5, label='src_class_decision')\n",
    "    ax1.plot(labels.data.detach().cpu().numpy(),'r', alpha=0.5, label='src_class_labels')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.set_title('train', fontsize=20)\n",
    "\n",
    "    data = src_val_loader.dataset.data.to(device)\n",
    "    labels = src_val_loader.dataset.labels.to(device).long()\n",
    "    feature_out, class_out, _ = model(data)\n",
    "    out_sigmoid = torch.sigmoid(class_out).data.detach().cpu().numpy()\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    ax2.plot(out_sigmoid[:,1],'.b', label='tgt_class_sigmoid', markersize=3)\n",
    "    ax2.plot(out_sigmoid[:,1].round(),'b', alpha=0.5, label='tgt_class_decision')\n",
    "    ax2.plot(labels.data.detach().cpu().numpy(),'r', alpha=0.5, label='tgt_class_labels')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_title('val', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "src_name:  UPFall_neck\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (426, 3, 66)\n",
      "val_data shape: (132, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UPFall_neck 0.7938144329896907\n",
      "\n",
      "\n",
      "src_name:  UPFall_wrist\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (426, 3, 66)\n",
      "val_data shape: (132, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UPFall_wrist 0.7731958762886598\n",
      "\n",
      "\n",
      "src_name:  UPFall_belt\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (426, 3, 66)\n",
      "val_data shape: (132, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UPFall_belt 0.8298969072164949\n",
      "\n",
      "\n",
      "src_name:  UPFall_rightpocket\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (393, 3, 66)\n",
      "val_data shape: (99, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UPFall_rightpocket 0.7430167597765364\n",
      "\n",
      "\n",
      "src_name:  UPFall_ankle\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (426, 3, 66)\n",
      "val_data shape: (132, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UPFall_ankle 0.6288659793814433\n",
      "UPFall_ankle is bad src\n",
      "\n",
      "\n",
      "src_name:  UMAFall_chest\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (355, 3, 66)\n",
      "val_data shape: (88, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UMAFall_chest 0.9574468085106383\n",
      "\n",
      "\n",
      "src_name:  UMAFall_wrist\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (356, 3, 66)\n",
      "val_data shape: (88, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UMAFall_wrist 0.8794326241134752\n",
      "\n",
      "\n",
      "src_name:  UMAFall_waist\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (344, 3, 66)\n",
      "val_data shape: (87, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UMAFall_waist 0.8439716312056738\n",
      "\n",
      "\n",
      "src_name:  UMAFall_leg\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (356, 3, 66)\n",
      "val_data shape: (88, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "UMAFall_leg 0.75177304964539\n",
      "\n",
      "\n",
      "src_name:  UMAFall_ankle\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (295, 3, 66)\n",
      "val_data shape: (88, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../falldetect/training_util.py:79: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  F1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAFall_ankle 0.21052631578947367\n",
      "UMAFall_ankle is bad src\n",
      "\n",
      "\n",
      "src_name:  SFDLA_chest\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (2528, 3, 66)\n",
      "val_data shape: (587, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "SFDLA_chest 0.968705547652916\n",
      "\n",
      "\n",
      "src_name:  SFDLA_wrist\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (2525, 3, 66)\n",
      "val_data shape: (587, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "SFDLA_wrist 0.9608262108262108\n",
      "\n",
      "\n",
      "src_name:  SFDLA_waist\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (2528, 3, 66)\n",
      "val_data shape: (587, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "SFDLA_waist 0.9900426742532006\n",
      "\n",
      "\n",
      "src_name:  SFDLA_thigh\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (2528, 3, 66)\n",
      "val_data shape: (587, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "SFDLA_thigh 0.980796586059744\n",
      "\n",
      "\n",
      "src_name:  SFDLA_ankle\n",
      "------------------------------Working on i_CV 1------------------------------\n",
      "Working on get_data_loader...\n",
      "train_data shape: (2525, 3, 66)\n",
      "val_data shape: (588, 3, 66)\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "SFDLA_ankle 0.9508896797153025\n",
      "['UPFall_ankle', 'UMAFall_ankle']\n",
      "train results\n",
      "{   'SFDLA_ankle': 0.9508896797153025,\n",
      "    'SFDLA_chest': 0.968705547652916,\n",
      "    'SFDLA_thigh': 0.980796586059744,\n",
      "    'SFDLA_waist': 0.9900426742532006,\n",
      "    'SFDLA_wrist': 0.9608262108262108,\n",
      "    'UMAFall_ankle': 0.21052631578947367,\n",
      "    'UMAFall_chest': 0.9574468085106383,\n",
      "    'UMAFall_leg': 0.75177304964539,\n",
      "    'UMAFall_waist': 0.8439716312056738,\n",
      "    'UMAFall_wrist': 0.8794326241134752,\n",
      "    'UPFall_ankle': 0.6288659793814433,\n",
      "    'UPFall_belt': 0.8298969072164949,\n",
      "    'UPFall_neck': 0.7938144329896907,\n",
      "    'UPFall_rightpocket': 0.7430167597765364,\n",
      "    'UPFall_wrist': 0.7731958762886598}\n",
      "val results\n",
      "{   'SFDLA_ankle': 0.9305993690851735,\n",
      "    'SFDLA_chest': 0.9053627760252366,\n",
      "    'SFDLA_thigh': 0.9272151898734177,\n",
      "    'SFDLA_waist': 0.9968354430379747,\n",
      "    'SFDLA_wrist': 0.930379746835443,\n",
      "    'UMAFall_ankle': 0.525,\n",
      "    'UMAFall_chest': 0.95,\n",
      "    'UMAFall_leg': 0.975,\n",
      "    'UMAFall_waist': 0.8205128205128205,\n",
      "    'UMAFall_wrist': 0.85,\n",
      "    'UPFall_ankle': 0.6666666666666666,\n",
      "    'UPFall_belt': 0.75,\n",
      "    'UPFall_neck': 0.85,\n",
      "    'UPFall_rightpocket': 0.8222222222222222,\n",
      "    'UPFall_wrist': 0.9833333333333333}\n"
     ]
    }
   ],
   "source": [
    "bad_src_names_list = []\n",
    "# results = {}\n",
    "results_train = {}\n",
    "# [src_name] =  train_performance_dict_list[epoch]['sensitivity']\n",
    "results_val = {}\n",
    "# [src_name] =  val_src_performance_dict_list[epoch]['src_sensitivity']\n",
    "    \n",
    "for src_name in src_names:\n",
    "    print('\\n\\nsrc_name: ', src_name)\n",
    "# src_name = src_names[0]\n",
    "\n",
    "#     if not os.path.exists(outputdir):\n",
    "#         os.makedirs(outputdir)\n",
    "\n",
    "    # TODO: don't need to extract training_params\n",
    "    classes_n = training_params['classes_n']\n",
    "    CV_n = training_params['CV_n']\n",
    "    num_epochs = training_params['num_epochs']\n",
    "    channel_n = training_params['channel_n']\n",
    "    batch_size = training_params['batch_size']\n",
    "    learning_rate = training_params['learning_rate']\n",
    "    extractor_type = training_params['extractor_type']\n",
    "    device = training_params['device']\n",
    "    show_diagnosis_plt = training_params['show_diagnosis_plt']\n",
    "\n",
    "    src_dataset_name = src_name.split('_')[0]\n",
    "    src_sensor_loc = src_name.split('_')[1]\n",
    "\n",
    "\n",
    "    src_inputdir = inputdir + '{}/{}/'.format(src_dataset_name, src_sensor_loc)\n",
    "\n",
    "\n",
    "    print('------------------------------Working on i_CV {}------------------------------'.format(i_CV))\n",
    "    # 1. prepare dataset\n",
    "    src_train_loader, src_val_loader = get_data_loader(src_inputdir, i_CV, batch_size, learning_rate)\n",
    "    # tgt_train_loader, tgt_val_loader = get_data_loader(tgt_inputdir, i_CV, batch_size, learning_rate)\n",
    "\n",
    "    # the model expect the same input dimension for src and tgt data\n",
    "    src_train_size = src_train_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "    src_val_size = src_val_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "\n",
    "    # tgt_train_size = tgt_train_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "    # tgt_val_size = tgt_val_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "\n",
    "    src_input_dim = src_train_loader.dataset.data.data.detach().cpu().numpy().shape[2]\n",
    "    # tgt_input_dim = tgt_train_loader.dataset.data.data.detach().cpu().numpy().shape[2]\n",
    "\n",
    "    # 2. prepare model\n",
    "\n",
    "    total_step = len(src_train_loader)\n",
    "\n",
    "    train_performance_dict_list = list( {} for i in range(num_epochs) )\n",
    "    val_src_performance_dict_list = list( {} for i in range(num_epochs) )\n",
    "    # val_tgt_performance_dict_list = list( {} for i in range(num_epochs) )\n",
    "\n",
    "    # model = DannModel(device, class_N=classes_n, domain_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "    if extractor_type == 'CNN':\n",
    "        model = DannModel2(device, class_N=classes_n, domain_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "#         model = DannModel(device, class_N=classes_n, domain_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "    elif extractor_type == 'CNNLSTM':\n",
    "        dropout = training_params['dropout']\n",
    "        hiddenDim_f = training_params['hiddenDim_f']\n",
    "        hiddenDim_y = training_params['hiddenDim_y']\n",
    "        hiddenDim_d = training_params['hiddenDim_d']\n",
    "        win_size = training_params['win_size']\n",
    "        win_stride = training_params['win_stride']\n",
    "        step_n = training_params['step_n']\n",
    "        model = CnnLstm(device, class_N=classes_n, channel_n=channel_n, dropout=dropout, hiddenDim_f=hiddenDim_f, hiddenDim_y=hiddenDim_y, hiddenDim_d=hiddenDim_d, win_size=win_size, win_stride=win_stride, step_n=step_n).to(device)\n",
    "\n",
    "    model_name = model.__class__.__name__\n",
    "    # loss and optimizer\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "#     plot_epoch(src_train_loader, src_val_loader, model)\n",
    "\n",
    "\n",
    "    # 3. fit the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_performance_dict = train_epoch(src_train_loader, device, model, class_criterion, optimizer, epoch)\n",
    "        train_performance_dict_list[epoch] = train_performance_dict\n",
    "\n",
    "        val_src_performance_dict = val_epoch(src_val_loader, device, model, class_criterion, optimizer, epoch, 'src')\n",
    "        val_src_performance_dict_list[epoch] = val_src_performance_dict\n",
    "\n",
    "#     plot_epoch(src_train_loader, src_val_loader, model)\n",
    "    print(src_name, train_performance_dict_list[epoch]['sensitivity'])\n",
    "    results_train[src_name] =  train_performance_dict_list[epoch]['sensitivity']\n",
    "    results_val[src_name] =  val_src_performance_dict_list[epoch]['src_sensitivity']\n",
    "    \n",
    "    if train_performance_dict_list[epoch]['sensitivity'] < 0.7:\n",
    "        print('{} is bad src'.format(src_name))\n",
    "        bad_src_names_list.append(src_name)\n",
    "        \n",
    "#         print('train results')\n",
    "#         pp = pprint.PrettyPrinter(indent=4)\n",
    "#         pp.pprint(train_performance_dict_list[epoch])\n",
    "#         print('val results')\n",
    "#         pp = pprint.PrettyPrinter(indent=4)\n",
    "#         pp.pprint(val_src_performance_dict_list[epoch])\n",
    "        \n",
    "print(bad_src_names_list)\n",
    "print('train results')\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(results_train)\n",
    "print('val results')\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(results_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amhwi5lZJA5V"
   },
   "source": [
    "# functions developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jb6CCtPOfoxO"
   },
   "outputs": [],
   "source": [
    "# # validated, the implementation is correct\n",
    "# def contextExapansion(x, win_size, win_overlap, step_n):\n",
    "#   # size of x: torch.Size([batch_size, channel_n, input_size])\n",
    "#   # size of x_seq: torch.Size([step_n, batch_size, channel_n, win_size])\n",
    "\n",
    "#   batch_size = x.size()[0]\n",
    "#   channel_n = x.size()[1]\n",
    "#   input_size = x.size()[2]\n",
    "\n",
    "#   x_seq = torch.ones((step_n, batch_size, channel_n, win_size), dtype=torch.double)\n",
    "#   timesteps = np.asarray(range(win_size))\n",
    "#   for i in range(step_n):\n",
    "#     indices = i*win_size*win_overlap+timesteps\n",
    "#     x_seq[i, :, :, :] = x[:,:,indices]\n",
    "#   return x_seq\n",
    "\n",
    "# def labelExapansion(y, step_n):\n",
    "#   # size of y: torch.Size([batch_size, 1])\n",
    "#   # size of y_seq: torch.Size([batch_size, step_n])\n",
    "\n",
    "#   batch_size = y.size()[0]\n",
    "\n",
    "#   y_seq = torch.ones((step_n, batch_size), dtype=torch.double)\n",
    "#   timesteps = np.asarray(range(win_size))\n",
    "#   for i in range(step_n):\n",
    "#     y_seq[i, :] = y\n",
    "#   return y_seq\n",
    "\n",
    "# win_size=22\n",
    "# win_overlap=0.5\n",
    "# step_n = 5\n",
    "# x = torch.zeros((16,3,66), dtype=torch.double)\n",
    "# y = torch.zeros((16,), dtype=torch.double)\n",
    "# x_seq = contextExapansion(x, win_size, win_overlap, step_n)\n",
    "# y_seq = labelExapansion(y, step_n)\n",
    "# print(x.size(), x_seq.size(), y_seq.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8FqrD37E4tI"
   },
   "outputs": [],
   "source": [
    "# # validated, the implementation is correct\n",
    "# def contextExapansion_v2(x, win_size, win_stride, step_n):\n",
    "#   # size of x: torch.Size([batch_size, channel_n, input_size])\n",
    "#   # size of x_seq: torch.Size([step_n, batch_size, channel_n, win_size])\n",
    "\n",
    "#   batch_size = x.size()[0]\n",
    "#   channel_n = x.size()[1]\n",
    "#   input_size = x.size()[2]\n",
    "\n",
    "#   x_seq = torch.ones((step_n, batch_size, channel_n, win_size), dtype=torch.double)\n",
    "#   timesteps = np.asarray(range(win_size))\n",
    "#   for i in range(step_n):\n",
    "#     indices = i*win_stride+timesteps\n",
    "#     x_seq[i, :, :, :] = x[:,:,indices]\n",
    "#   return x_seq\n",
    "\n",
    "# def labelExapansion(y, step_n):\n",
    "#   # size of y: torch.Size([batch_size, 1])\n",
    "#   # size of y_seq: torch.Size([batch_size, step_n])\n",
    "\n",
    "#   batch_size = y.size()[0]\n",
    "\n",
    "#   y_seq = torch.ones((step_n, batch_size), dtype=torch.double)\n",
    "#   timesteps = np.asarray(range(win_size))\n",
    "#   for i in range(step_n):\n",
    "#     y_seq[i, :] = y\n",
    "#   return y_seq\n",
    "\n",
    "# win_size=24\n",
    "# win_stride=7\n",
    "# step_n = 7\n",
    "# x = torch.zeros((16,3,66), dtype=torch.double)\n",
    "# y = torch.zeros((16,), dtype=torch.double)\n",
    "# x_seq = contextExapansion_v2(x, win_size, win_stride, step_n)\n",
    "# y_seq = labelExapansion(y, step_n)\n",
    "# print(x.size(), x_seq.size(), y_seq.size(), y.size())\n",
    "# # print(x_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOWRU-84_8_y"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # fall classifier neural network (fc layers)\n",
    "# class ClassClassifier_lstm(nn.Module):\n",
    "#   def __init__(self, num_classes=2, hiddenDim=16, input_dim=50, steps_n=5):\n",
    "#       super(ClassClassifier_lstm, self).__init__()\n",
    "#       self.lstm = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "#         input_size=input_dim,\n",
    "#         hidden_size=hiddenDim,         # rnn hidden unit\n",
    "#         num_layers=2,           # number of rnn layer\n",
    "#         batch_first=False,       # input & output will has not batch size as 1st dimension. e.g. (time_step, time_step, input_size)\n",
    "#         bidirectional=True,\n",
    "#       )\n",
    "#       self.fc1 = nn.Linear(steps_n*hiddenDim*2, 10)\n",
    "#       self.fc2 = nn.Linear(10, num_classes)\n",
    "#       self.relu = nn.ReLU(inplace=False)\n",
    "#       self.fc3 = nn.Linear(steps_n*hiddenDim*2, num_classes)\n",
    "\n",
    "#       # self.lsm = nn.LogSoftmax(dim=1)\n",
    "      \n",
    "#   def forward(self, x):\n",
    "#     debug = False\n",
    "#     # Input: (seq_len, batch, input_size)\n",
    "#     # Output: (seq_len, batch, num_directions * hidden_size)\n",
    "#     out1_seq, (h_n, h_c) = self.lstm(x)\n",
    "#     out1_seq = out1_seq.transpose(0,1)\n",
    "#     out1_seq = out1_seq.reshape(out1_seq.size()[0],-1)\n",
    "\n",
    "#     out2 = self.relu(self.fc1(out1_seq))\n",
    "#     out3 = self.fc2(out2)\n",
    "#     # out3 = self.fc3(out1_seq)\n",
    "\n",
    "#     if debug:\n",
    "#       print('ClassClassifier_lstm')\n",
    "#       print('out1_seq size:', out1_seq.size())\n",
    "#       print('out2 size:', out2.size())\n",
    "#       print('out3 size:', out3.size())\n",
    "\n",
    "#     return out3\n",
    "\n",
    "# # domain classifier neural network (fc layers)\n",
    "# class DomainClassifier_lstm(nn.Module):\n",
    "#   def __init__(self, num_classes=2, hiddenDim=16, input_dim=50, steps_n=5):\n",
    "#       super(DomainClassifier_lstm, self).__init__()\n",
    "#       self.lstm = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "#         input_size=input_dim,\n",
    "#         hidden_size=hiddenDim,         # rnn hidden unit\n",
    "#         num_layers=2,           # number of rnn layer\n",
    "#         batch_first=False,       # input & output will has not batch size as 1st dimension. e.g. (time_step, time_step, input_size)\n",
    "#         bidirectional=True,\n",
    "#       )\n",
    "#       self.fc1 = nn.Linear(steps_n*hiddenDim*2, 10)\n",
    "#       self.fc2 = nn.Linear(10, num_classes)\n",
    "#       self.relu = nn.ReLU(inplace=False)\n",
    "#       # self.lsm = nn.LogSoftmax(dim=1)\n",
    "      \n",
    "#   def forward(self, x, constant):\n",
    "#     debug = False\n",
    "\n",
    "#     x = GradReverse.grad_reverse(x.float(), constant)\n",
    "\n",
    "#     out1_seq, (h_n, h_c) = self.lstm(x)\n",
    "#     out1_seq = out1_seq.transpose(0,1)\n",
    "#     out1_seq = out1_seq.reshape(out1_seq.size()[0],-1)\n",
    "#     # print('out1_seq', out1_seq)\n",
    "\n",
    "#     out2 = self.relu(self.fc1(out1_seq))\n",
    "#     out3 = self.fc2(out2)\n",
    "#     # out3 = self.lsm(self.fc2(out2))\n",
    "#     # print('out3', out3)\n",
    "\n",
    "#     if debug:\n",
    "#       print('DomainClassifier_lstm')\n",
    "#       print('out1_seq size:', out1_seq.size())\n",
    "#       print('out2 size:', out2.size())\n",
    "#       print('out3 size:', out3.size())\n",
    "      \n",
    "#     return out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tVO5_IcnVys"
   },
   "outputs": [],
   "source": [
    "# # Convolutional neural network (two convolutional layers)\n",
    "# class CnnLstm(nn.Module):\n",
    "#   def __init__(self, device, class_N=2, channel_n=16, dropout=0.5, hiddenDim_f=5, hiddenDim_y=5, hiddenDim_d=5, win_size=22, win_overlap=0.5, step_n=5):\n",
    "#       super(CnnLstm, self).__init__()\n",
    "#       self.win_size = win_size\n",
    "#       self.win_overlap = win_overlap\n",
    "#       self.step_n = step_n\n",
    "#       self.feature_extractor = FeatureExtractor(input_dim=win_size, channel_n=channel_n).to(device).float()\n",
    "\n",
    "#       cnn_layer1_dim = (win_size+2*2-1*(3-1)-1)+1\n",
    "#       pool_layer1_dim = (cnn_layer1_dim-1*(2-1)-1)/2+1\n",
    "#       cnn_layer2_dim = (pool_layer1_dim+2*2-1*(3-1)-1)+1\n",
    "#       pool_layer2_dim = (cnn_layer2_dim-1*(2-1)-1)/2+1\n",
    "#       self.feature_out_dim = int(pool_layer2_dim*channel_n*2)\n",
    "\n",
    "#       # lstm_out_seq size: torch.Size([step_n, batch_size, hiddenDim*2])\n",
    "#       self.lstm = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "#         input_size=self.feature_out_dim,\n",
    "#         hidden_size=hiddenDim_f,         # rnn hidden unit\n",
    "#         num_layers=2,           # number of rnn layer\n",
    "#         batch_first=False,       # input & output will has not batch size as 1st dimension. e.g. (time_step, batch, input_size)\n",
    "#         bidirectional=True,\n",
    "#         dropout=dropout\n",
    "#       ).to(device).float()\n",
    "\n",
    "\n",
    "#       # self.class_classifier = ClassClassifier_lstm(num_classes=2, hiddenDim=hiddenDim_y, input_dim=hiddenDim_f*2, steps_n=step_n).to(device).float()\n",
    "#       # self.domain_classifier = DomainClassifier_lstm(num_classes=2, hiddenDim=hiddenDim_d, input_dim=hiddenDim_f*2, steps_n=step_n).to(device).float()\n",
    "#       self.class_classifier = ClassClassifier_lstm(num_classes=2, hiddenDim=hiddenDim_y, input_dim=self.feature_out_dim, steps_n=step_n).to(device).float()\n",
    "#       self.domain_classifier = DomainClassifier_lstm(num_classes=2, hiddenDim=hiddenDim_d, input_dim=self.feature_out_dim, steps_n=step_n).to(device).float()\n",
    "      \n",
    "\n",
    "      \n",
    "#   def forward(self, x):\n",
    "#     # size of x: torch.Size([batch_size, channel_n, input_size])\n",
    "#     # size of x_seq: torch.Size([step_n, batch_size, channel_n, win_size])\n",
    "#     debug = False\n",
    "#     x_seq = contextExapansion(x, self.win_size, self.win_overlap, self.step_n).to(device).float()\n",
    "#     # aaa = x_seq[0,0,:,:].cpu().numpy().T\n",
    "#     # bbb = x_seq[1,0,:,:].cpu().numpy().T\n",
    "\n",
    "#     # for i in range(5):\n",
    "#     #   plt.subplot(5, 1, i+1)\n",
    "#     #   plt.plot(x_seq[i,0,:,:].cpu().numpy().T)\n",
    "#     #   plt.title('t={}'.format(i))\n",
    "#     #   # plt.ylabel('t=0')\n",
    "#     # plt.show()\n",
    "\n",
    "#     feature_out_seq = torch.ones((self.step_n, x.size()[0], self.feature_out_dim), dtype=torch.float).to(device)\n",
    "\n",
    "#     for t in range(self.step_n):\n",
    "#       # Input: (N, C_in, L_in)\n",
    "#       # Output: (N, L_out=self.feature_out_dim*C_out)\n",
    "#       feature_out_seq[t,:,:] = self.feature_extractor(x_seq[t,:,:,:])\n",
    "    \n",
    "#     lstm_out_seq = feature_out_seq\n",
    "#     # Input: (seq_len, batch, input_size)\n",
    "#     # Output: (seq_len, batch, num_directions * hidden_size)\n",
    "#     # lstm_out_seq, (h_n, h_c) = self.lstm(feature_out_seq, None)\n",
    "\n",
    "#     # print(lstm_out_seq)\n",
    "#     # sys.exit()\n",
    "#     class_output = self.class_classifier(lstm_out_seq)\n",
    "#     domain_output = self.domain_classifier(lstm_out_seq, 1)\n",
    "\n",
    "#     if debug:\n",
    "#       print('CnnLstm')\n",
    "#       print('x size:', x.size())\n",
    "#       print('x_seq size:', x_seq.size())\n",
    "#       print('feature_out_seq size:', feature_out_seq.size())\n",
    "\n",
    "#       print('lstm_out_seq size:', lstm_out_seq.size())\n",
    "#       print('class_output size:', class_output.size())\n",
    "#       print('domain_output size:', domain_output.size())\n",
    "\n",
    "#     lstm_out_seq = lstm_out_seq.transpose(0,1)\n",
    "#     lstm_out_seq = lstm_out_seq.reshape(lstm_out_seq.size()[0],-1)\n",
    "\n",
    "#     # print('lstm_out_seq', lstm_out_seq)\n",
    "#     return lstm_out_seq, class_output, domain_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJn8YBlofouQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpitQXMDlURn"
   },
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# # device = torch.device('cpu')\n",
    "# print('show GPU device name:', torch.cuda.get_device_name(0))\n",
    "# model_1 = FeatureExtractor(input_dim=66).to(device).float()\n",
    "\n",
    "# # test_input = torch.randn((8, 3, 66), dtype=torch.double)\n",
    "\n",
    "# # test_input = test_input.to(device)\n",
    "#     # labels = labels.to(device).long()\n",
    "\n",
    "# feature_out = model_1(test_input)\n",
    "# print('show model_1 output size:', feature_out.size())\n",
    "\n",
    "# feature_out.data.detach().cpu().numpy()\n",
    "\n",
    "# feature_out_dim =  feature_out.size()[1]\n",
    "# model_2 = ClassClassifier(num_classes=2, input_dim=feature_out_dim).to(device).float()\n",
    "# model_3 = DomainClassifier(num_classes=2, input_dim=feature_out_dim).to(device).float()\n",
    "\n",
    "# model_2_out = model_2(feature_out)\n",
    "# print('show model_2 output size:', model_2_out.size())\n",
    "\n",
    "# model_3_out = model_3(feature_out, 1)\n",
    "# print('show model_3 output size:', model_3_out.size())\n",
    "\n",
    "# model_4 = CascadedModel(model_1, model_2)\n",
    "# # model_4 = nn.Sequential(model_1, model_2)\n",
    "\n",
    "# print('model_4 output size', model_4(test_input).size())\n",
    "\n",
    "# dann = DannModel(device, class_N=2, domain_N=2, channel_n=5, input_dim=66).to(device).float()\n",
    "# feature_out, class_output, domain_output = dann(test_input)\n",
    "# print('dann output size', feature_out.size(), class_output.size(), domain_output.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ah2MW37CCWtR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1avdeEhyCWrS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNIrQ3McJFn0"
   },
   "source": [
    "\n",
    "# testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1585579968792,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "9QdNcq3kZLHg",
    "outputId": "115224d3-e5d9-4b95-ae78-a3f7a91083bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win_size, cnn_layer1_dim, pool_layer1_dim, cnn_layer2_dim, pool_layer2_dim size: 18 20 10.0 12.0 6.0\n"
     ]
    }
   ],
   "source": [
    "win_size = 18\n",
    "\n",
    "cnn_layer1_dim = (win_size+2*2-1*(3-1)-1)+1\n",
    "pool_layer1_dim = (cnn_layer1_dim-1*(2-1)-1)/2+1\n",
    "cnn_layer2_dim = (pool_layer1_dim+2*2-1*(3-1)-1)+1\n",
    "pool_layer2_dim = (cnn_layer2_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "print('win_size, cnn_layer1_dim, pool_layer1_dim, cnn_layer2_dim, pool_layer2_dim size:', win_size, cnn_layer1_dim, pool_layer1_dim, cnn_layer2_dim, pool_layer2_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1585579978388,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "8lu43fzrNDrN",
    "outputId": "505bfe76-4d99-4b9f-fed1-e04794627587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "test_input size: torch.Size([253, 3, 66])\n",
      "FeatureExtractor_total_params: 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchan2020/miniconda3/envs/FD_DAT/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnLstm_total_params: 27434\n",
      "CnnLstm(\n",
      "  (feature_extractor): FeatureExtractor(\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv1d(3, 4, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "      (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (class_classifier): ClassClassifier_lstm(\n",
      "    (lstm): LSTM(48, 5, dropout=0.5, bidirectional=True)\n",
      "    (fc1): Linear(in_features=90, out_features=10, bias=True)\n",
      "    (fc2): Linear(in_features=10, out_features=2, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (fc3): Linear(in_features=90, out_features=2, bias=True)\n",
      "  )\n",
      "  (domain_classifier): DomainClassifier_lstm(\n",
      "    (lstm): LSTM(48, 5, dropout=0.5, bidirectional=True)\n",
      "    (fc4): Linear(in_features=432, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU()\n",
      "    (fc5): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "FeatureExtractor_total_params: 168\n",
      "DannModel_total_params: 748\n",
      "torch.Size([253, 432])\n",
      "show lstm_out_seq, class_output, domain_output size: torch.Size([253, 432]) torch.Size([253, 2]) torch.Size([253, 2])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# print('show GPU device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "batch_size = 253\n",
    "axis_dim = 3\n",
    "input_size = 66\n",
    "test_input = torch.randn((batch_size, axis_dim, input_size), dtype=torch.double).to(device)\n",
    "print('test_input size:', test_input.size())\n",
    "\n",
    "channel_n = 4\n",
    "# model = CnnLstm(device, class_N=2, channel_n=channel_n, dropout=0.5, hiddenDim_f=5, hiddenDim_y=5, hiddenDim_d=5, win_size=22, win_stride=11, step_n=5).to(device)\n",
    "model = CnnLstm(device, class_N=2, channel_n=channel_n, dropout=0.5, hiddenDim_f=5, hiddenDim_y=5, hiddenDim_d=5, win_size=18, win_stride=6, step_n=9).to(device)\n",
    "print(model)\n",
    "lstm_out_seq, class_output, domain_output = model(test_input)\n",
    "\n",
    "dann = DannModel(device, class_N=2, domain_N=2, channel_n=channel_n, input_dim=66).to(device).float()\n",
    "\n",
    "print(lstm_out_seq.size())\n",
    "print('show lstm_out_seq, class_output, domain_output size:', lstm_out_seq.size(), class_output.size(), domain_output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1SMbbKoHXSgADadbJZEbaPjVutZfgpfpz"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 206667,
     "status": "ok",
     "timestamp": 1585582687878,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "MjvZC1mDEOYz",
    "outputId": "a61459bf-12cd-4f06-dcb3-e5cdfe59e400"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optimal\n",
    "\n",
    "# tasks_list = [('UMAFall_waist', 'UPFall_belt')]\n",
    "# tasks_list = [('UPFall_belt', 'UMAFall_waist')]\n",
    "# tasks_list = [('UPFall_belt', 'UMAFall_waist')]\n",
    "tasks_list = [('UMAFall_ankle', 'UPFall_ankle')]\n",
    "# tasks_list = [('UMAFall_wrist', 'UPFall_wrist')]\n",
    "# tasks_list = [('UMAFall_leg', 'UPFall_rightpocket')]\n",
    "\n",
    "CV_n = 3\n",
    "num_epochs = 10\n",
    "extractor_type = 'CNNLSTM'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "training_params = {\n",
    "  'classes_n': 2,\n",
    "  'CV_n': CV_n,\n",
    "  'num_epochs': num_epochs,\n",
    "  'channel_n': 16,\n",
    "  'batch_size': 16,\n",
    "  'learning_rate': 0.001,\n",
    "  'extractor_type': extractor_type,\n",
    "  'device': device,\n",
    "  'dropout': 0.5,\n",
    "  'hiddenDim_f': 5,\n",
    "  'hiddenDim_y': 5,\n",
    "  'hiddenDim_d': 5,\n",
    "  'win_size': 18,\n",
    "  'win_stride': 6,\n",
    "  'step_n': 9,\n",
    "  }\n",
    "\n",
    "# for i, training_params in enumerate(training_params_list):\n",
    "df_performance_table = pd.DataFrame('', index=['source', 'DANN', 'target', 'domain'], columns=[])\n",
    "\n",
    "for task_item in tasks_list:\n",
    "  start_time = time.time()\n",
    "\n",
    "  (src_name, tgt_name) = task_item\n",
    "\n",
    "  inputdir = '/content/drive/My Drive/中研院/data_mic/stage1_preprocessed_18hz_LOO/'\n",
    "  outputdir = '/content/drive/My Drive/中研院/data_mic/stage2_modeloutput_18hz_LOO_testing/{}_{}/'.format(src_name, tgt_name)\n",
    "  if not os.path.exists(outputdir):\n",
    "      os.makedirs(outputdir)\n",
    "  print('outputdir for stage2 output:', outputdir)\n",
    "\n",
    "  df_performance_table = performance_table(df_performance_table, src_name, tgt_name, training_params, inputdir, outputdir)\n",
    "\n",
    "  time_elapsed = time.time() - start_time\n",
    "  print('time elapsed:', time_elapsed)\n",
    "  df_performance_table.loc['time_elapsed'] = time_elapsed\n",
    "\n",
    "  df_outputdir = outputdir\n",
    "\n",
    "  print('df_performance_table saved at', df_outputdir)\n",
    "  df_performance_table.to_csv(df_outputdir+'df_performance_table_optimal.csv', encoding='utf-8')\n",
    "\n",
    "  display(df_performance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 136272,
     "status": "ok",
     "timestamp": 1585578044955,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "l6KnxpN4ecTM",
    "outputId": "ae90ef25-7817-4174-b445-7e7d2373fd6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UMAFall_ankle_UPFall_ankle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>0.333±0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DANN</th>\n",
       "      <td>0.263±0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.838±0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain</th>\n",
       "      <td>0.794±0.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_elapsed</th>\n",
       "      <td>134.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             UMAFall_ankle_UPFall_ankle\n",
       "source                      0.333±0.109\n",
       "DANN                        0.263±0.245\n",
       "target                      0.838±0.046\n",
       "domain                      0.794±0.328\n",
       "time_elapsed                     134.21"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_performance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBWFQ68-CWo8"
   },
   "outputs": [],
   "source": [
    "# # training_params = {\n",
    "# #   'classes_n': 2,\n",
    "# #   'CV_n': 2,\n",
    "# #   'num_epochs': 10,\n",
    "# #   'channel_n': 4,\n",
    "# #   'batch_size': 32,\n",
    "# #   'learning_rate': 0.01,\n",
    "# #   'extractor_type': 'CNN',}\n",
    "# training_params = {\n",
    "#   'classes_n': 2,\n",
    "#   'CV_n': 2,\n",
    "#   'num_epochs': 10,\n",
    "#   'channel_n': 4,\n",
    "#   'batch_size': 32,\n",
    "#   'learning_rate': 0.01,\n",
    "#   'extractor_type': 'CNNLSTM',\n",
    "#   'dropout': 0,\n",
    "#   'hiddenDim_f': 5,\n",
    "#   'hiddenDim_y': 5,\n",
    "#   'hiddenDim_d': 5,\n",
    "#   'win_size': 22,\n",
    "#   'win_overlap': 0.5,\n",
    "#   'step_n': 5,\n",
    "#   }\n",
    "\n",
    "# tasks_list = [('UMAFall_leg', 'UPFall_rightpocket')]\n",
    "# (src_name, tgt_name) = tasks_list[0]\n",
    "# inputdir = '/content/drive/My Drive/中研院/data_mic/stage1_preprocessed_18hz_LOO/'\n",
    "# outputdir = '/content/drive/My Drive/中研院/data_mic/stage2_modeloutput_18hz_LOO_testing/{}_{}/'.format(src_name, tgt_name)\n",
    "\n",
    "# source_outputs = BaselineModel_fitting_2(training_params, src_name, tgt_name, inputdir, outputdir+'source/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4eUtQpb4JRCy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEJIJK9wJRAs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tbn2zdvRJQ96"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvZq_sZhJQ6y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "soPxNFfuJRx3"
   },
   "source": [
    "# fixing DannModel_fitting WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84EbLjuPCWms"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def DannModel_fitting(training_params, src_name, tgt_name, inputdir, outputdir): \n",
    "\n",
    "#   if not os.path.exists(outputdir):\n",
    "#       os.makedirs(outputdir)\n",
    "\n",
    "#   classes_n = training_params['classes_n']\n",
    "#   CV_n = training_params['CV_n']\n",
    "#   num_epochs = training_params['num_epochs']\n",
    "#   channel_n = training_params['channel_n']\n",
    "#   batch_size = training_params['batch_size']\n",
    "#   learning_rate = training_params['learning_rate']\n",
    "\n",
    "#   df_performance = pd.DataFrame(0, index=np.arange(CV_n), \n",
    "#                                 columns=['i_CV',\n",
    "#                                         'train_src_class_loss','train_tgt_class_loss','train_src_domain_loss','train_tgt_domain_loss', \n",
    "#                                         'train_src_class_acc','train_tgt_class_acc','train_domain_acc',\n",
    "#                                         'val_src_class_loss','val_tgt_class_loss','val_src_domain_loss','val_tgt_domain_loss',\n",
    "#                                         'val_src_class_acc','val_tgt_class_acc','val_domain_acc'])\n",
    "\n",
    "#   src_dataset_name = src_name.split('_')[0]\n",
    "#   src_sensor_loc = src_name.split('_')[1]\n",
    "\n",
    "#   tgt_dataset_name = tgt_name.split('_')[0]\n",
    "#   tgt_sensor_loc = tgt_name.split('_')[1]\n",
    "\n",
    "#   src_inputdir = inputdir + '{}/{}/'.format(src_dataset_name, src_sensor_loc)\n",
    "#   tgt_inputdir = inputdir + '{}/{}/'.format(tgt_dataset_name, tgt_sensor_loc)\n",
    "\n",
    "\n",
    "#   for i_CV in range(CV_n):\n",
    "#     print('------------------------------Working on i_CV {}------------------------------'.format(i_CV))\n",
    "#     # 1. prepare dataset\n",
    "#     src_train_loader, src_val_loader = get_UMAFall_loader(src_inputdir, i_CV, batch_size, learning_rate)\n",
    "#     tgt_train_loader, tgt_val_loader = get_UPFall_loader(tgt_inputdir, i_CV, batch_size, learning_rate)\n",
    "\n",
    "#     # the model expect the same input dimension for src and tgt data\n",
    "#     src_train_size = src_train_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "#     src_val_size = src_val_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "\n",
    "#     tgt_train_size = tgt_train_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "#     tgt_val_size = tgt_val_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "\n",
    "#     src_input_dim = src_train_loader.dataset.data.data.detach().cpu().numpy().shape[2]\n",
    "#     tgt_input_dim = tgt_train_loader.dataset.data.data.detach().cpu().numpy().shape[2]\n",
    "\n",
    "#     # 2. prepare model\n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     # loss and optimizer\n",
    "#     class_criterion = nn.CrossEntropyLoss()\n",
    "#     domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # 3. fit the model\n",
    "#     total_step = len(src_train_loader)\n",
    "\n",
    "#     train_loss_avg_epochs = np.zeros(num_epochs)\n",
    "#     train_src_class_acc_epochs = np.zeros(num_epochs)\n",
    "#     train_tgt_class_acc_epochs = np.zeros(num_epochs)\n",
    "#     train_domain_acc = np.zeros(num_epochs)\n",
    "#     val_loss_avg_epochs = np.zeros(num_epochs)\n",
    "#     val_src_class_acc_epochs = np.zeros(num_epochs)\n",
    "#     val_tgt_class_acc_epochs = np.zeros(num_epochs)\n",
    "#     val_domain_acc = np.zeros(num_epochs)\n",
    "\n",
    "#     df_performance.loc[i_CV,'i_CV'] = i_CV\n",
    "#     model = DannModel(device, class_N=classes_n, domain_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "#     model_name = model.__class__.__name__\n",
    "#     train_size = src_train_size+tgt_train_size\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "#     model_output_diagnosis_trainval(model, src_train_loader, tgt_train_loader, src_val_loader, tgt_val_loader, device, '_epoch{}'.format(0), i_CV, outputdir)\n",
    "#     model_features_diagnosis_trainval(model, src_train_loader, tgt_train_loader, src_val_loader, tgt_val_loader, device, '_epoch{}'.format(0), i_CV, outputdir)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#       fitting_outputs = train_epoch_dann(src_train_loader, tgt_train_loader, src_train_size, tgt_train_size, device, \n",
    "#                                           model, \n",
    "#                                           class_criterion, domain_criterion, optimizer, epoch)\n",
    "      \n",
    "#       train_loss_avg, src_class_loss_avg, tgt_class_loss_avg, src_domain_loss_avg, tgt_domain_loss_avg, src_class_acc, tgt_class_acc, domain_acc = fitting_outputs\n",
    "\n",
    "#       train_loss_avg_epochs[epoch] = train_loss_avg\n",
    "#       train_src_class_acc_epochs[epoch] = src_class_acc\n",
    "#       train_tgt_class_acc_epochs[epoch] = tgt_class_acc\n",
    "#       train_domain_acc[epoch] = domain_acc\n",
    "#       df_performance.loc[i_CV,['train_src_class_loss','train_tgt_class_loss','train_src_domain_loss','train_tgt_domain_loss', \n",
    "#                                 'train_src_class_acc','train_tgt_class_acc','train_domain_acc']] = [src_class_loss_avg, tgt_class_loss_avg, src_domain_loss_avg, tgt_domain_loss_avg, src_class_acc, tgt_class_acc, domain_acc]\n",
    "\n",
    "#       val_outputs = val_epoch_dann(src_val_loader, tgt_val_loader, src_val_size, tgt_val_size, device, \n",
    "#                                       model,\n",
    "#                                       class_criterion, domain_criterion, epoch)\n",
    "\n",
    "#       val_loss_avg, src_class_loss_avg, tgt_class_loss_avg, src_domain_loss_avg, tgt_domain_loss_avg, src_class_acc, tgt_class_acc, domain_acc = val_outputs\n",
    "\n",
    "#       val_loss_avg_epochs[epoch] = val_loss_avg\n",
    "#       val_src_class_acc_epochs[epoch] = src_class_acc\n",
    "#       val_tgt_class_acc_epochs[epoch] = tgt_class_acc\n",
    "#       val_domain_acc[epoch] = domain_acc\n",
    "\n",
    "#       # 4. store the performance of the model at the last epoch\n",
    "#       df_performance.loc[i_CV,['val_src_class_loss','val_tgt_class_loss','val_src_domain_loss','val_tgt_domain_loss', \n",
    "#                                 'val_src_class_acc','val_tgt_class_acc','val_domain_acc']] = [src_class_loss_avg, tgt_class_loss_avg, src_domain_loss_avg, tgt_domain_loss_avg, src_class_acc, tgt_class_acc, domain_acc]\n",
    "    \n",
    "#     dann_learning_diagnosis(num_epochs, train_loss_avg_epochs, val_loss_avg_epochs, \\\n",
    "#     train_src_class_acc_epochs, val_src_class_acc_epochs, \\\n",
    "#     train_tgt_class_acc_epochs, val_tgt_class_acc_epochs, \\\n",
    "#     train_domain_acc, val_domain_acc, i_CV, outputdir)\n",
    "    \n",
    "#     print('-----------------Exporting pytorch model-----------------')\n",
    "#     loaded_model = DannModel(device, class_N=classes_n, domain_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "#     export_model(model, loaded_model, outputdir+'model_CV{}'.format(i_CV))\n",
    "\n",
    "#     print('-----------------Evaluating trained model-----------------')\n",
    "#     model_output_diagnosis_trainval(loaded_model, src_train_loader, tgt_train_loader, src_val_loader, tgt_val_loader, device, '_epoch{}'.format(epoch), i_CV, outputdir)\n",
    "#     model_features_diagnosis_trainval(loaded_model, src_train_loader, tgt_train_loader, src_val_loader, tgt_val_loader, device, '_epoch{}'.format(epoch), i_CV, outputdir)\n",
    "\n",
    "#   # 5. export model performance as df\n",
    "#   print('---------------Exporting model performance---------------')\n",
    "#   export_perofmance(df_performance, CV_n, outputdir)\n",
    "\n",
    "#   print('val_src_class_acc: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['val_src_class_acc'], df_performance.loc['std']['val_src_class_acc']))\n",
    "#   print('val_tgt_class_acc: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['val_tgt_class_acc'], df_performance.loc['std']['val_tgt_class_acc']))\n",
    "#   print('val_domain_acc: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['val_domain_acc'], df_performance.loc['std']['val_domain_acc']))\n",
    "\n",
    "#   # print('=========================================================')\n",
    "\n",
    "#   # 6. export notebook parameters as dict\n",
    "#   # datetime object containing current date and time\n",
    "#   print('--------------Exporting notebook parameters--------------')\n",
    "#   now = datetime.now()\n",
    "#   dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "#   samples_n = src_train_size + src_val_size\n",
    "\n",
    "#   param_dict = {\n",
    "#       'CV_n': CV_n,\n",
    "#       'samples_n': samples_n,\n",
    "#       'classes_n': classes_n,\n",
    "#       'model_name': model_name,\n",
    "#       'src_dataset_name': src_dataset_name,\n",
    "#       'tgt_dataset_name': tgt_dataset_name,\n",
    "#       'src_sensor_loc': src_sensor_loc,\n",
    "#       'tgt_sensor_loc': tgt_sensor_loc,\n",
    "#       'date': dt_string,\n",
    "#       'num_epochs': num_epochs,\n",
    "#       'channel_n': channel_n,\n",
    "#       'batch_size': batch_size,\n",
    "#       'learning_rate': learning_rate,\n",
    "#       'input_dim': (batch_size, src_train_loader.dataset.data.size()[1], src_train_loader.dataset.data.size()[2]),\n",
    "#       'output_dim': 2,\n",
    "#       'label_dim': src_train_loader.dataset.labels[0:batch_size].data.detach().cpu().numpy().shape,\n",
    "#   }\n",
    "#   print(param_dict)\n",
    "\n",
    "#   with open(outputdir+'notebook_param.json', 'w') as fp:\n",
    "#     json.dump(param_dict, fp)\n",
    "\n",
    "#   print('val_tgt_class_acc: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['val_tgt_class_acc'], df_performance.loc['std']['val_tgt_class_acc']))\n",
    "#   print('val_domain_acc: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['val_domain_acc'], df_performance.loc['std']['val_domain_acc']))\n",
    "\n",
    "#   return (df_performance.loc['mean']['val_tgt_class_acc'], df_performance.loc['std']['val_tgt_class_acc']), (df_performance.loc['mean']['val_domain_acc'], df_performance.loc['std']['val_domain_acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGUsfWNDCWkq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oy_4AJnWCWiR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1e5p1K_-pVsH"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision.models\n",
    "# import collections\n",
    "# import math\n",
    "# import sys\n",
    "\n",
    "# class lstmnet(nn.Module):\n",
    "#     r\"\"\"lstmnet is a simple recurrent neural network that contains one \n",
    "#     hidden layer of 64 nodes with 3 time steps by default. It expects \n",
    "#     an input of 3D tensor with a dimension (batch, time_step, \n",
    "#     input_size). The output will be a 2D tensor with a dimension (N, 2).\n",
    "\n",
    "#     Args:\n",
    "#         - None. The variables used for each sub-layer are hard-coded\n",
    "#     Shape:\n",
    "#         - Input: :math:`(batch, time_step, input_size)`\n",
    "#         - Output: :math:`(batch, output_size)`\n",
    "#     Examples::\n",
    "#         >>> m = lstmnet()\n",
    "#         >>> batchSize = 16\n",
    "#         >>> featDim = 10\n",
    "#         >>> timeStep = 3\n",
    "#         >>> input = torch.randn(batchSize, timeStep, featDim)\n",
    "#         >>> output = m(input)\n",
    "#         >>> output.size()\n",
    "#             (16, 2)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, inputDim=10, hiddenDim=64, outputDim=2, ):\n",
    "#         super(lstmnet, self).__init__()\n",
    "#         self.outputDim = outputDim\n",
    "\n",
    "#         self.lstm = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "#             input_size=int((inputDim-2)/3),\n",
    "#             hidden_size=hiddenDim,         # rnn hidden unit\n",
    "#             num_layers=2,           # number of rnn layer\n",
    "#             batch_first=True,       # input & output will has batch size as 1st dimension. e.g. (batch, time_step, input_size)\n",
    "#             bidirectional=True,\n",
    "#             dropout=0.5\n",
    "#         )\n",
    "\n",
    "#         self.fc1 = nn.Linear(hiddenDim*2+2, 25)\n",
    "#         self.fc2 = nn.Linear(25, outputDim)\n",
    "#         self.relu = nn.ReLU(inplace=False)\n",
    "#         self.lsm = nn.LogSoftmax(dim=1)\n",
    "# #         self.bn = nn.BatchNorm1d(10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#       # x shape (time_step, batch, input_size, channel_n), float tensor\n",
    "#       # r_out shape (time_step, batch, lstm_output_size)\n",
    "#       # h_n shape (n_layers, batch, hidden_size)\n",
    "#       # h_c shape (n_layers, batch, hidden_size)\n",
    "#       # out shape (batch, output_size)\n",
    "#       x = x.float()\n",
    "\n",
    "#       timestep_n = x.size()[0]\n",
    "#       batch_n = x.size()[1]\n",
    "#       input_size = x.size()[2]\n",
    "#       out1_size = x.size()[2]\n",
    "#       channel_n =  x.size()[3]\n",
    "      \n",
    "#       out1 = torch.randn(timestep_n, , , dtype=torch.double)\n",
    "\n",
    "#       # None represents zero initial hidden state, so don't have to implement initHidden\n",
    "#       for timestep in range(timestep_n:\n",
    "#         out1[timestep,:,:,:] = self.layer1(x[timestep,:,:,:])\n",
    "\n",
    "\n",
    "#         out_freq_x, (h_n, h_c) = self.lstm(x[:,:,:], None)\n",
    "#         out_freq_y, (h_n, h_c) = self.lstm(x[:,:,100:200], None)\n",
    "#         out_freq_z, (h_n, h_c) = self.lstm(x[:,:,200:300], None)\n",
    "        \n",
    "#         out_flstm = out_freq_x + out_freq_y + out_freq_z # torch.Size([16, 5, 128])\n",
    "# #         print('size of out_flstm is ', out_flstm.size())\n",
    "\n",
    "#         out_cat = torch.cat((out_flstm, x[:,:,-2:]), 2)\n",
    "# #         out_cat = out_flstm\n",
    "\n",
    "#         out = torch.randn(x.size()[0], x.size()[1], self.outputDim, dtype=torch.double)\n",
    "\n",
    "#         # in this implementation, outputs from all timesteps are used for loss and back prop\n",
    "#         for timestep in range(out_cat.size()[1]):\n",
    "#             out_step = self.relu(self.fc1(out_cat[:, timestep, :]))\n",
    "#             out_step = self.fc2(out_step)\n",
    "# #             out_step = self.fc(out_cat[timestep, :, :])\n",
    "#             out[:, timestep, :] = self.lsm(out_step)\n",
    "            \n",
    "#         debug = False\n",
    "#         if debug == True:\n",
    "#             print('-----------------------------')\n",
    "#             print('size of x is ', x.size())\n",
    "#             print('size of out_freq_x is ', out_freq_x.size())\n",
    "#             print('size of h_n is ', h_n.size())\n",
    "#             print('size of h_c is ', h_c.size())\n",
    "#             print('size of out_cat is ', out_cat.size())\n",
    "#             print('size of out_step is ', out_step.size())\n",
    "#             print('size of out is ', out.size())\n",
    "#             print('-----------------------------')\n",
    "#             sys.exit()\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQXX-AwxpTuf"
   },
   "outputs": [],
   "source": [
    "# # Convolutional neural network (two convolutional layers)\n",
    "# class ConvNet2(nn.Module):\n",
    "#     def __init__(self, class_N=2, channel_n=16, input_dim=10, p=0.5):\n",
    "#         super(ConvNet2, self).__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv1d(3, channel_n, kernel_size=3, stride=1, padding=2),\n",
    "#             nn.BatchNorm1d(channel_n),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv1d(channel_n, channel_n, kernel_size=3, stride=1, padding=2),\n",
    "#             nn.BatchNorm1d(channel_n),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "#         # self.layer3 = nn.Sequential(\n",
    "#         #     nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "#         #     nn.BatchNorm1d(64),\n",
    "#         #     nn.ReLU(),\n",
    "#         #     nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "#         cnn_layer1_dim = (input_dim+2*2-1*(3-1)-1)+1\n",
    "#         pool_layer1_dim = (cnn_layer1_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "#         # cnn_layer2_dim = (pool_layer1_dim+2*2-1*(3-1)-1)+1\n",
    "#         # pool_layer2_dim = (cnn_layer2_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "#         # cnn_layer3_dim = (pool_layer2_dim+2*2-1*(3-1)-1)+1\n",
    "#         # pool_layer3_dim = (cnn_layer3_dim-1*(2-1)-1)/2+1\n",
    "\n",
    "#         # print('cnn_layer1_dim:', cnn_layer1_dim)\n",
    "#         # print('pool_layer1_dim:', pool_layer1_dim)\n",
    "#         # print('cnn_layer2_dim:', cnn_layer2_dim)\n",
    "#         # print('pool_layer2_dim:', pool_layer2_dim)\n",
    "#         # print('cnn_layer3_dim:', cnn_layer3_dim)\n",
    "#         # print('pool_layer3_dim:', pool_layer3_dim)\n",
    "#         # fc_dim = int(((((input_dim)+2*2-1)/2+2*2-1)/2+2*2-1)/2*64)\n",
    "#         self.fc1 = nn.Linear(int(pool_layer1_dim)*channel_n, 50)\n",
    "#         self.drop_out = nn.Dropout(p=0)\n",
    "#         self.fc2 = nn.Linear(50, class_N)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#       out1 = self.layer1(x.float())\n",
    "#       # print('out1 size:', out1.size())\n",
    "#       # out2 = self.layer2(out1)\n",
    "#       # print('out2 size:', out2.size())\n",
    "#       # out3 = self.layer3(out2)\n",
    "#       # print('out3 size:', out3.size())\n",
    "#       # print('out2 size:', out2.size())\n",
    "#       out1 = out1.reshape(out1.size(0), -1)\n",
    "#       # print('out2 size:', out2.size())\n",
    "#       out1 = self.drop_out(out1)\n",
    "#       out2 = self.fc1(out1)\n",
    "#       out3 = self.fc2(out2)\n",
    "#       # print('x, out1, out2, out 3, out4 size',  x.size(), out1.size(), out2.size(), out3.size(), out4.size())\n",
    "#       return out1, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLJNv0EXEfH-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuLbnqA6Xwgm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yCS6WL4kSGL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqlC22yVW6hp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5zQmLdUGkBw"
   },
   "outputs": [],
   "source": [
    "# def train_epoch(train_loader, train_size, device, model, criterion, optimizer, epoch):\n",
    "#   total_train_loss = 0\n",
    "#   train_TPTF = 0\n",
    "#   debug = False\n",
    "#   for i, (data, labels) in enumerate(train_loader):\n",
    "\n",
    "#     data = data.to(device)\n",
    "#     labels = labels.to(device).long()\n",
    "\n",
    "#     # Forward pass\n",
    "#     # feature_out, class_out = model(data)\n",
    "#     feature_out, class_out, _ = model(data)\n",
    "\n",
    "#     train_loss = criterion(class_out, labels)\n",
    "\n",
    "#     # Backward and optimize\n",
    "#     optimizer.zero_grad()\n",
    "#     train_loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # total_train_loss += train_loss.data.numpy()\n",
    "#     total_train_loss += train_loss.data.detach().cpu().numpy()\n",
    "#     out_sigmoid = torch.sigmoid(class_out).data.detach().cpu().numpy()\n",
    "#     train_pred = np.argmax(out_sigmoid, 1)\n",
    "#     train_TPTF += (train_pred==labels.data.detach().cpu().numpy()).sum()\n",
    "\n",
    "#     #######################\n",
    "#     if debug:\n",
    "#       print('Epoch [{}/{}] Step [{}/{}]:'\n",
    "#             'train_loss={:.5f} train_acc={:.5f}'\n",
    "#             .format(epoch + 1,\n",
    "#                     20,\n",
    "#                     i + 1,\n",
    "#                     len(train_loader),\n",
    "#                     train_loss,\n",
    "#                     (train_pred==labels.data.detach().cpu().numpy()).sum()))\n",
    "#     #######################\n",
    "                \n",
    "#   train_loss = total_train_loss/train_size\n",
    "#   train_acc = train_TPTF/train_size\n",
    "\n",
    "#   return train_loss, train_acc\n",
    "\n",
    "# def val_epoch(val_loader, val_size, device, model, criterion, optimizer, epoch):\n",
    "#   total_val_loss = 0\n",
    "#   val_TPTF = 0\n",
    "#   debug = False\n",
    "  \n",
    "#   for i, (data, labels) in enumerate(val_loader):\n",
    "#     data = data.to(device)\n",
    "#     labels = labels.to(device).long()\n",
    "    \n",
    "#     #Forward pass\n",
    "#     # feature_out, class_out = model(data)\n",
    "#     feature_out, class_out, _ = model(data)\n",
    "#     val_loss = criterion(class_out, labels)\n",
    "    \n",
    "#     total_val_loss += val_loss.data.detach().cpu().numpy()\n",
    "#     out_sigmoid = torch.sigmoid(class_out).data.detach().cpu().numpy()\n",
    "#     val_pred = np.argmax(out_sigmoid, 1)\n",
    "#     val_TPTF += (val_pred==labels.data.detach().cpu().numpy()).sum()\n",
    "\n",
    "#     #######################\n",
    "#     if debug:\n",
    "#       print('Epoch [{}/{}] Step [{}/{}]:'\n",
    "#             'val_loss={:.5f} val_acc={:.5f}'\n",
    "#             .format(epoch + 1,\n",
    "#                     20,\n",
    "#                     i + 1,\n",
    "#                     len(val_loader),\n",
    "#                     val_loss,\n",
    "#                     (val_pred==labels.data.detach().cpu().numpy()).sum()))\n",
    "#     #######################\n",
    "\n",
    "#   val_loss = total_val_loss/val_size\n",
    "#   val_acc = val_TPTF/val_size\n",
    "\n",
    "#   return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3-u-rG8Gj_d"
   },
   "outputs": [],
   "source": [
    "# def ConvNet2Model_fitting(training_params, src_name, tgt_name, inputdir, outputdir): \n",
    "#   show_train_log = False\n",
    "\n",
    "#   if not os.path.exists(outputdir):\n",
    "#       os.makedirs(outputdir)\n",
    "      \n",
    "#   classes_n = training_params['classes_n']\n",
    "#   CV_n = training_params['CV_n']\n",
    "#   num_epochs = training_params['num_epochs']\n",
    "#   channel_n = training_params['channel_n']\n",
    "#   batch_size = training_params['batch_size']\n",
    "#   learning_rate = training_params['learning_rate']\n",
    "#   dropout_p = training_params['dropout_p']\n",
    "\n",
    "#   df_performance = pd.DataFrame(columns=['i_CV',\n",
    "#                                           'train_loss','train_acc','val_loss','val_acc', 'tgt_val_loss', 'tgt_val_acc'])\n",
    "\n",
    "#   src_dataset_name = src_name.split('_')[0]\n",
    "#   src_sensor_loc = src_name.split('_')[1]\n",
    "\n",
    "#   tgt_dataset_name = tgt_name.split('_')[0]\n",
    "#   tgt_sensor_loc = tgt_name.split('_')[1]\n",
    "\n",
    "#   src_inputdir = inputdir + '{}/{}/'.format(src_dataset_name, src_sensor_loc)\n",
    "#   tgt_inputdir = inputdir + '{}/{}/'.format(tgt_dataset_name, tgt_sensor_loc)\n",
    "\n",
    "\n",
    "#   for i_CV in range(CV_n):\n",
    "#     # 1. prepare dataset\n",
    "#     src_train_loader, src_val_loader = get_UMAFall_loader(src_inputdir, i_CV, batch_size, learning_rate)\n",
    "#     tgt_train_loader, tgt_val_loader = get_UPFall_loader(tgt_inputdir, i_CV, batch_size, learning_rate)\n",
    "\n",
    "#     # the model expect the same input dimension for src and tgt data\n",
    "#     src_train_size = src_train_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "#     src_val_size = src_val_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "\n",
    "#     tgt_train_size = tgt_train_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "#     tgt_val_size = tgt_val_loader.dataset.data.data.detach().cpu().numpy().shape[0]\n",
    "\n",
    "#     src_input_dim = src_train_loader.dataset.data.data.detach().cpu().numpy().shape[2]\n",
    "#     tgt_input_dim = tgt_train_loader.dataset.data.data.detach().cpu().numpy().shape[2]\n",
    "\n",
    "#     # 2. prepare model\n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     # loss and optimizer\n",
    "#     # criterion = nn.CrossEntropyLoss()\n",
    "#     class_criterion = nn.CrossEntropyLoss()\n",
    "#     # domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#     # 3. fit the model\n",
    "#     total_step = len(src_train_loader)\n",
    "\n",
    "#     train_loss_avg_epochs = np.zeros(num_epochs)\n",
    "#     train_class_acc_epochs = np.zeros(num_epochs)\n",
    "#     val_src_loss_avg_epochs = np.zeros(num_epochs)\n",
    "#     val_src_class_acc_epochs = np.zeros(num_epochs)\n",
    "#     val_tgt_loss_avg_epochs = np.zeros(num_epochs)\n",
    "#     val_tgt_class_acc_epochs = np.zeros(num_epochs)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "\n",
    "#       # if training_mode == 'source':\n",
    "#       # model = BaselineModel(device, class_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "#       # model = ConvNet2(class_N=2, channel_n=16, input_dim=66, p=dropout_p).to(device).float()\n",
    "#       model = DannModel(device, class_N=2, domain_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "#       model_name = model.__class__.__name__\n",
    "#       optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "#       # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#       train_loss, train_acc = train_epoch(src_train_loader, src_train_size, device, model, class_criterion, optimizer, epoch)\n",
    "#       train_loss_avg_epochs[epoch] = train_loss\n",
    "#       train_class_acc_epochs[epoch] = train_acc\n",
    "\n",
    "#       val_loss, val_acc = val_epoch(src_val_loader, src_val_size, device, model, class_criterion, optimizer, epoch)\n",
    "#       val_src_loss_avg_epochs[epoch] = val_loss\n",
    "#       val_src_class_acc_epochs[epoch] = val_acc\n",
    "\n",
    "#       tgt_val_loss, tgt_val_acc = val_epoch(tgt_val_loader, tgt_val_size, device, model, class_criterion, optimizer, epoch)\n",
    "#       val_tgt_loss_avg_epochs[epoch] = tgt_val_loss\n",
    "#       val_tgt_class_acc_epochs[epoch] = tgt_val_acc\n",
    "\n",
    "#       if show_train_log:\n",
    "#         print('Epoch {}'.format(epoch))\n",
    "#         print('Train Loss: {:.6f}, Train ACC: {:.6f}, Val loss = {:.6f}, Val ACC: {:.6f}'.\n",
    "#               format(train_loss, train_acc, val_loss, val_acc))\n",
    "#         print('Target Val loss = {:.6f}, Val ACC: {:.6f}'.format(tgt_val_loss, tgt_val_acc))\n",
    "\n",
    "#       # 4. store the performance of the model at the last epoch\n",
    "#       df_performance.loc[i_CV] = [i_CV, train_loss, train_acc, val_loss, val_acc, tgt_val_loss, tgt_val_acc]\n",
    "    \n",
    "#     fig = plt.figure(figsize=(10, 3), dpi=80)\n",
    "#     ax1 = fig.add_subplot(1, 2, 1)\n",
    "#     ax1.set_title('loss_avg_epochs')\n",
    "#     ax1.set_xlabel('epoch')\n",
    "#     ax1.plot(np.arange(num_epochs), train_loss_avg_epochs, color='blue', label='train')\n",
    "#     ax1.plot(np.arange(num_epochs), val_src_loss_avg_epochs, color='red', label='val_src')\n",
    "#     ax1.plot(np.arange(num_epochs), val_tgt_loss_avg_epochs, color='green', label='val_tgt')\n",
    "#     ax1.legend(loc=\"upper right\")\n",
    "#     ax2 = fig.add_subplot(1, 2, 2)\n",
    "#     ax2.set_title('class_acc_epochs')\n",
    "#     ax2.set_xlabel('epoch')\n",
    "#     ax2.plot(np.arange(num_epochs), train_class_acc_epochs, color='blue', label='train')\n",
    "#     ax2.plot(np.arange(num_epochs), val_src_class_acc_epochs, color='red', label='val_src')\n",
    "#     ax2.plot(np.arange(num_epochs), val_tgt_class_acc_epochs, color='green', label='val_tgt')\n",
    "#     ax2.legend(loc=\"upper right\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     print('=================Exporting pytorch model=================')\n",
    "#     # loaded_model = ConvNet2(class_N=2, channel_n=16, input_dim=66, p=dropout_p).to(device).float()\n",
    "#     loaded_model = DannModel(device, class_N=2, domain_N=2, channel_n=channel_n, input_dim=src_input_dim).to(device).float()\n",
    "#     export_model(model, loaded_model, outputdir+'model_CV{}'.format(i_CV))\n",
    "#     print('=========================================================')\n",
    "\n",
    "#   # 5. export model performance as df\n",
    "#   print('===============Exporting model performance===============')\n",
    "#   export_perofmance(df_performance, CV_n, outputdir)\n",
    "\n",
    "#   print('src val loss: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['val_loss'], df_performance.loc['std']['val_loss']))\n",
    "#   print('src val acc: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['val_acc'], df_performance.loc['std']['val_acc']))\n",
    "  \n",
    "#   print('tgt val loss: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['tgt_val_loss'], df_performance.loc['std']['tgt_val_loss']))\n",
    "#   print('tgt val acc: {:.4f}±{:.4f}'.format(df_performance.loc['mean']['tgt_val_acc'], df_performance.loc['std']['tgt_val_acc']))\n",
    "\n",
    "#   print('=========================================================')\n",
    "\n",
    "#   # 6. export notebook parameters as dict\n",
    "#   # datetime object containing current date and time\n",
    "#   print('==============Exporting notebook parameters==============')\n",
    "#   now = datetime.now()\n",
    "#   dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "#   samples_n = src_train_size + src_val_size\n",
    "\n",
    "#   param_dict = {\n",
    "#       'CV_n': CV_n,\n",
    "#       'samples_n': samples_n,\n",
    "#       'classes_n': classes_n,\n",
    "#       'model_name': model_name,\n",
    "#       'dataset_name': src_dataset_name,\n",
    "#       'sensor_loc': src_sensor_loc,\n",
    "#       'date': dt_string,\n",
    "#       'batch_size': batch_size,\n",
    "#       'input_dim': (batch_size, src_train_loader.dataset.data.size()[1], src_train_loader.dataset.data.size()[2]),\n",
    "#       'output_dim': src_train_loader.dataset.labels[0:batch_size].data.detach().cpu().numpy().shape,\n",
    "#       'label_dim': CV_n,\n",
    "#   }\n",
    "#   print(param_dict)\n",
    "\n",
    "#   with open(outputdir+'notebook_param.json', 'w') as fp:\n",
    "#     json.dump(param_dict, fp)\n",
    "#   print('=========================================================')\n",
    "\n",
    "#   return (df_performance.loc['mean']['val_acc'], df_performance.loc['std']['val_acc']), (df_performance.loc['mean']['tgt_val_acc'], df_performance.loc['std']['tgt_val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gfQua3gGj83"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwNPLlA8HoWr"
   },
   "outputs": [],
   "source": [
    "# # tasks_list = [('UMAFall_wrist', 'UPFall_wrist')]\n",
    "# tasks_list = [('UMAFall_waist', 'UPFall_belt')]\n",
    "\n",
    "# # optimal_training_params = {\n",
    "# #     'classes_n': 2,\n",
    "# #     'CV_n': 5,\n",
    "# #     'num_epochs': 3,\n",
    "# #     'channel_n': 32,\n",
    "# #     'batch_size': 1,\n",
    "# #     'learning_rate': 0.01}\n",
    "\n",
    "# training_params = {\n",
    "#     'classes_n': 2,\n",
    "#     'CV_n': 5,\n",
    "#     'num_epochs': 20,\n",
    "#     'channel_n': 2,\n",
    "#     'batch_size': 1,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'dropout_p': 0.2}\n",
    "\n",
    "# for task_item in tasks_list:\n",
    "#   (src_name, tgt_name) = task_item\n",
    "\n",
    "#   inputdir = '/content/drive/My Drive/中研院/data_mic/stage1_preprocessed_18hz/'\n",
    "#   outputdir = '/content/drive/My Drive/中研院/data_mic/stage2_archdesign_18hz/{}_{}/'.format(src_name, tgt_name)\n",
    "#   if not os.path.exists(outputdir):\n",
    "#       os.makedirs(outputdir)\n",
    "#   print('outputdir for stage2 output:', outputdir)\n",
    "  \n",
    "#   source_outputs = ConvNet2Model_fitting(training_params, src_name, tgt_name, inputdir, outputdir+'source/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FUoXUwbHoS4"
   },
   "outputs": [],
   "source": [
    "# source_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A66bXfcDHoP8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mfs_IpRkHoLm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRx6_kUvHoIE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDVBXu0i4ZNB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2Z4tKb2_0Nq"
   },
   "source": [
    "# Start CV training and validation in a big phat loop (to be deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZlGwYngxAhw"
   },
   "outputs": [],
   "source": [
    "# def model_fitting(CV_n, classes_n, sensor_loc, dataset_name, inputdir): \n",
    "#   # it's big phat loop i don't like it qq\n",
    "#   df_performance = pd.DataFrame(columns=['i_CV','train_loss','train_acc','val_loss','val_acc'])\n",
    "\n",
    "#   for i_CV in range(CV_n):\n",
    "#     # 1. prepare dataset\n",
    "#     train_inputdir = inputdir+'/CV{}/train'.format(i_CV)\n",
    "#     val_inputdir = inputdir+'/CV{}/val'.format(i_CV)\n",
    "\n",
    "#     train_data = data_loader('data', train_inputdir).transpose(2,1,0)\n",
    "#     val_data = data_loader('data', val_inputdir).transpose(2,1,0)\n",
    "\n",
    "#     train_labels = data_loader('labels', train_inputdir)\n",
    "#     val_labels = data_loader('labels', val_inputdir)\n",
    "\n",
    "#     train_i_sub = data_loader('i_sub', train_inputdir)\n",
    "#     val_i_sub = data_loader('i_sub', val_inputdir)\n",
    "\n",
    "#     print('train_data shape:', train_data.shape)\n",
    "#     print('val_data shape:', val_data.shape)\n",
    "\n",
    "#     train_size = train_labels.shape[0]\n",
    "#     val_size = val_labels.shape[0]\n",
    "#     input_dim = train_data.shape[2]\n",
    "\n",
    "#     # convert labels from multi-class activities to binary (fall/ADL)\n",
    "#     train_labels_binary = ((train_labels==10)|(train_labels==11)|(train_labels==12)).astype(int)\n",
    "#     val_labels_binary = ((val_labels==10)|(val_labels==11)|(val_labels==12)).astype(int)\n",
    "\n",
    "#     train_dataset = FallDataset(train_data, train_labels_binary)\n",
    "#     val_dataset = FallDataset(val_data, val_labels_binary)\n",
    "#     # data loader\n",
    "#     batch_size = 4\n",
    "#     learning_rate = 0.001\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "#                                               batch_size=batch_size, \n",
    "#                                               shuffle=True)\n",
    "\n",
    "#     val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "#                                               batch_size=batch_size, \n",
    "#                                               shuffle=False)\n",
    "\n",
    "#     # 2. prepare model\n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     model = ConvNet(num_classes=classes_n, input_dim=input_dim).to(device).float()\n",
    "\n",
    "#     # loss and optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     # test model on a batch\n",
    "#     try:\n",
    "#       out = model(train_dataset.data[0:batch_size,:,:])\n",
    "#       model_outdim = out.data.numpy().shape\n",
    "#     except:\n",
    "#       print('Warning: model cannot read input')\n",
    "\n",
    "#     print('{} model architecture: '.format(model.__class__.__name__))\n",
    "#     print(model)\n",
    "\n",
    "\n",
    "#     # 3. fit the model\n",
    "#     num_epochs = 10\n",
    "#     total_step = len(train_loader)\n",
    "#     for epoch in range(num_epochs):\n",
    "#       total_train_loss = 0\n",
    "#       train_TPTF = 0\n",
    "#       for i, (data, labels) in enumerate(train_loader):\n",
    "#         data = data.to(device)\n",
    "#         labels = labels.to(device).long()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(data)\n",
    "#         train_loss = criterion(outputs, labels)\n",
    "#         total_train_loss += train_loss.data.numpy()\n",
    "        \n",
    "#         out_sigmoid = torch.sigmoid(outputs).data.numpy()\n",
    "#         train_pred = np.argmax(out_sigmoid, 1)\n",
    "#         train_TPTF += (train_pred==labels.data.numpy()).sum()\n",
    "#         # train_pred = print(np.argmax(F.sigmoid(outputs))\n",
    "\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         train_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # if (i+1) % 5 == 0:\n",
    "#         #     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.8f}' \n",
    "#         #             .format(epoch+1, num_epochs, i+1, total_step, train_loss.data.numpy()/labels.size()[0]))\n",
    "\n",
    "#       total_val_loss = 0\n",
    "#       val_TPTF = 0\n",
    "#       for i, (data, labels) in enumerate(val_loader):\n",
    "#         data = data.to(device)\n",
    "#         labels = labels.to(device).long()\n",
    "        \n",
    "#         #Forward pass\n",
    "#         val_outputs = model(data)\n",
    "#         val_loss = criterion(val_outputs, labels)\n",
    "#         total_val_loss += val_loss.data.numpy()\n",
    "\n",
    "#         out_sigmoid = torch.sigmoid(val_outputs).data.numpy()\n",
    "#         val_pred = np.argmax(out_sigmoid, 1)\n",
    "#         val_TPTF += (val_pred==labels.data.numpy()).sum()\n",
    "#         # print(val_TPTF, len(val_loader))\n",
    "          \n",
    "#       train_loss = total_train_loss/train_size\n",
    "#       train_acc = train_TPTF/train_size\n",
    "#       val_loss = total_val_loss/val_size\n",
    "#       val_acc = val_TPTF/val_size\n",
    "\n",
    "\n",
    "#       print('Epoch {}'.format(epoch+1))\n",
    "#       print('Train Loss: {:.6f}, Train ACC: {:.6f}, Val loss = {:.6f}, Val ACC: {:.6f}'.\n",
    "#             format(train_loss, train_acc, val_loss, val_acc))\n",
    "    \n",
    "#     # 4. store the performance of the model at the last epoch\n",
    "#     df_performance.loc[i_CV] = [i_CV, train_loss, train_acc, val_loss, val_acc]\n",
    "\n",
    "#   outputdir = '/content/drive/My Drive/中研院/data_mic/stage2_modeloutput/{}/{}/'.format(dataset_name, sensor_loc)\n",
    "#   if not os.path.exists(outputdir):\n",
    "#       os.makedirs(outputdir)\n",
    "#   print('outputdir for stage2 output:', outputdir)\n",
    "\n",
    "#   # 5. export model performance as df\n",
    "#   export_perofmance(df_performance, CV_n, outputdir)\n",
    "\n",
    "#   # 6. export notebook parameters as dict\n",
    "#   # datetime object containing current date and time\n",
    "#   now = datetime.now()\n",
    "#   dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "#   samples_n = train_size + val_size\n",
    "\n",
    "#   param_dict = {\n",
    "#       'CV_n': CV_n,\n",
    "#       'samples_n': samples_n,\n",
    "#       'classes_n': classes_n,\n",
    "#       'model_name': model.__class__.__name__,\n",
    "#       'dataset_name': dataset_name,\n",
    "#       'sensor_loc': sensor_loc,\n",
    "#       'date': dt_string,\n",
    "#       'batch_size': batch_size,\n",
    "#       'input_dim': (batch_size, train_dataset.data.size()[1], train_dataset.data.size()[2]),\n",
    "#       'output_dim': train_dataset.labels[0:batch_size].data.numpy().shape,\n",
    "#       'label_dim': CV_n,\n",
    "#   }\n",
    "#   print(param_dict)\n",
    "\n",
    "#   with open(outputdir+'notebook_param.json', 'w') as fp:\n",
    "#     json.dump(param_dict, fp)\n",
    "\n",
    "#   export_model(model, classes_n, input_dim, device, outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me8Xckeynq79"
   },
   "outputs": [],
   "source": [
    "# datasets_sensor_dict = {\n",
    "#     'UMAFall': ['waist', 'wrist', 'leg', 'chest', 'ankle'],\n",
    "#     'UPFall': ['wrist', 'rightpocket', 'neck', 'belt', 'ankle']\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuNFe4Q_oOyE"
   },
   "outputs": [],
   "source": [
    "# inputdir = '/content/drive/My Drive/中研院/data_mic/stage1_preprocessed/{}/{}/'.format(dataset_name, sensor_loc)\n",
    "# classes_n = 2\n",
    "# CV_n = 5\n",
    "\n",
    "# for key in datasets_sensor_dict.keys():\n",
    "#   dataset_name = key\n",
    "#   for sensor_loc in datasets_sensor_dict[dataset_name]:\n",
    "#     model_fitting(CV_n, classes_n, sensor_loc, dataset_name, inputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4I2Bn31oZ0l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PT0nxNaGqxim"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnK88jJ3kXd3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTuPZryA0XDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BIaUPJ-0XBz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWcFYNkc1S12"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOn/a8GYIoFJV7tUKKo5tXT",
   "collapsed_sections": [
    "amhwi5lZJA5V"
   ],
   "name": "stage2_ArchDesignStudio.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (FD_DAT)",
   "language": "python",
   "name": "fd_dat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
