{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Isjqqa84yJv-"
   },
   "source": [
    "**stage3_model_eval**. This notebook evaluates the trained model\n",
    "\n",
    "**Edit**<br/>\n",
    "\n",
    "**TODO**<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nswy3ke-TUyA"
   },
   "source": [
    "# Import packages and get authenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1585238820612,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "dR0gs1Ya0xmy",
    "outputId": "7cd142ac-9bd4-44c1-db43-2b84b7e35cdf"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/中研院/repo/')\n",
    "# sys.path.append('~/project_FDDAT/repo/')\n",
    "sys.path.append('../') # add this line so Data and data are visible in this file\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "from falldetect.utilities import *\n",
    "from falldetect.models import *\n",
    "from falldetect.dataset_util import *\n",
    "from falldetect.training_util import *\n",
    "from falldetect.eval_util import *\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# Plotting\n",
    "# checklist 1: comment inline, uncomment Agg\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFk7y6uzVQ2s"
   },
   "source": [
    "# Get user inputs\n",
    "In ipython notebook, these are hardcoded. In production python code, use parsers to provide these inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Js2lmEUEVQ-G"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='FD_DAT')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "# parser.add_argument('--extractor_type', metavar='extractor_type', help='extractor_type',\n",
    "#                     default='CNN')\n",
    "# parser.add_argument('--num_epochs', type=int, metavar='num_epochs', help='number of epochs',\n",
    "#                     default='5')\n",
    "# parser.add_argument('--CV_n', type=int, metavar='CV_n', help='CV folds',\n",
    "#                     default='2')\n",
    "# parser.add_argument('--rep_n', type=int, metavar='rep_n', help='number of repitition',\n",
    "#                     default='5')\n",
    "parser.add_argument('--tasks_list', metavar='tasks_list', help='a list of all tasks',\n",
    "                    default='UMAFall_waist_UPFall_belt UPFall_wrist_UMAFall_ankle')\n",
    "parser.add_argument('--src_names', metavar='src_names', help='a list of src_names',\n",
    "                    default='UMAFall_waist_UPFall_belt UPFall_wrist_UMAFall_ankle')\n",
    "parser.add_argument('--tgt_names', metavar='tgt_names', help='a list of tgt_names',\n",
    "                    default='UMAFall_waist_UPFall_belt UPFall_wrist_UMAFall_ankle')\n",
    "\n",
    "\n",
    "\n",
    "# split_mode = 'LOO'\n",
    "# split_mode = '5fold'\n",
    "\n",
    "# checklist 2: comment first line, uncomment second line seizures_FN\n",
    "args = parser.parse_args(['--input_folder', '../../data_mic/stage2/modeloutput_WithoutNormal_18hz_5fold_UPFall_UMAFall_cross-config_fixed', \n",
    "                          '--output_folder', '../../data_mic/stage3/UMAFall_UPFall_cross_config_fixed',\n",
    "                          '--tasks_list', 'UMAFall_chest-UPFall_neck UMAFall_wrist-UPFall_wrist UMAFall_waist-UPFall_belt UMAFall_leg-UPFall_rightpocket UMAFall_ankle-UPFall_ankle',\n",
    "                          '--src_names',  'UMAFall_chest UMAFall_wrist UMAFall_waist UMAFall_leg UMAFall_ankle',\n",
    "                          '--tgt_names',  'UPFall_neck UPFall_wrist UPFall_belt UPFall_rightpocket UPFall_ankle',\n",
    "                         ])\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = home+'/project_FDDAT/'\n",
    "input_folder = args.input_folder\n",
    "output_folder = args.output_folder\n",
    "\n",
    "tasks_list = []\n",
    "for item in args.tasks_list.split(' '):\n",
    "    tasks_list.append((item.split('-')[0], item.split('-')[1]))\n",
    "    \n",
    "src_domains = args.src_names.split(' ')\n",
    "tgt_domains = args.tgt_names.split(' ')\n",
    "\n",
    "inputdir = input_folder + '/'\n",
    "outputdir = output_folder + '/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task_metric(df_temp, metric_name, outputdir):\n",
    "    source_means = df_temp.loc['source',df_temp.columns != 'average'].apply(get_mean).values\n",
    "    DANN_means = df_temp.loc['DANN',df_temp.columns != 'average'].apply(get_mean).values\n",
    "    task_names = df_temp.columns[df_temp.columns != 'average']\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5), dpi=dpi)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.scatter(source_means, DANN_means, s=40, marker='o')\n",
    "    ax_xlim = ax.get_xlim()\n",
    "    ax_ylim = ax.get_ylim()\n",
    "    ax.plot([0, 1], [0, 1], c=\".3\", linewidth=1, alpha=0.4)\n",
    "    ax.set_title('{}'.format(metric_name.split('_')[1]), fontsize=20)\n",
    "    ax.set_xlabel('source_means', fontsize=15)\n",
    "    ax.set_ylabel('DANN_means', fontsize=15)   # relative to plt.rcParams['font.size']\n",
    "    ax.set_xlim(min(ax_xlim[0], ax_ylim[0]), max(ax_xlim[1], ax_ylim[1]))\n",
    "    ax.set_ylim(min(ax_xlim[0], ax_ylim[0]), max(ax_xlim[1], ax_ylim[1]))\n",
    "\n",
    "    for i, txt in enumerate(task_names):\n",
    "        ax.annotate(txt, (source_means[i], DANN_means[i]), fontsize=10, textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    fig.savefig(outputdir+'scatter_{}.png'.format(metric_name.split('_')[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_keys = ['df_acc', 'df_sensitivity', 'df_precision', 'df_F1']\n",
    "df_sample = pd.DataFrame(float('NaN'), columns=tgt_domains, index=src_domains)\n",
    "df_sample = df_sample.loc[:,~df_sample.columns.duplicated()]\n",
    "df_sample = df_sample.loc[~df_sample.index.duplicated(keep='first')]\n",
    "\n",
    "df_dict_cross_config = dict( zip(df_metric_keys,[df_sample.copy(), df_sample.copy(), df_sample.copy(), df_sample.copy()]))\n",
    "\n",
    "                                  \n",
    "training_setting_list = ['source', 'DANN', 'target', 'improvement']\n",
    "finetuned_results = dict( zip(training_setting_list,[copy.deepcopy(df_dict_cross_config), copy.deepcopy(df_dict_cross_config), copy.deepcopy(df_dict_cross_config), copy.deepcopy(df_dict_cross_config), ]))\n",
    "\n",
    "# df_task_list = []\n",
    "for task_item in tasks_list:\n",
    "    for training_setting in training_setting_list:\n",
    "        for metric_name in df_metric_keys:\n",
    "            if training_setting == 'improvement':\n",
    "                finetuned_results[training_setting][metric_name].at[src_name, tgt_name] = finetuned_results['DANN'][metric_name].at[src_name, tgt_name] - finetuned_results['source'][metric_name].at[src_name, tgt_name] \n",
    "                continue\n",
    "            (src_name, tgt_name) = task_item\n",
    "            df_task_inputdir = inputdir+src_name+'_'+tgt_name+'/repetitive_results/df_performance_table_agg_rep_{}.csv'.format(metric_name.split('_')[1])\n",
    "            if os.path.isfile(df_task_inputdir):\n",
    "                df_task = pd.read_csv(df_task_inputdir, index_col=0)[['rep_avg']].copy()\n",
    "            else:\n",
    "                continue\n",
    "            df_task.rename(columns={'rep_avg':src_name+'_'+tgt_name}, inplace=True)\n",
    "\n",
    "            finetuned_results[training_setting][metric_name].at[src_name, tgt_name] = float(df_task.copy().iloc[:, 0][training_setting].split('±')[0])\n",
    "\n",
    "for training_setting in finetuned_results.keys():\n",
    "    for metric_name in finetuned_results[training_setting].keys():\n",
    "        print('training_setting: {}, metric_name: {}'.format(training_setting, metric_name))\n",
    "        df_outputdir = '{}{}/'.format(outputdir, metric_name.split('_')[1])\n",
    "        if not os.path.exists(df_outputdir):\n",
    "            os.makedirs(df_outputdir)\n",
    "        print('will export data to', df_outputdir)\n",
    "\n",
    "        finetuned_results[training_setting][metric_name].round(5).to_csv(df_outputdir+'df_performance_table_agg_{}.csv'.format(training_setting), encoding='utf-8')\n",
    "        display(finetuned_results[training_setting][metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_list = dict( zip(df_metric_keys,[[], [], [], []]))\n",
    "\n",
    "for task_item in tasks_list:\n",
    "    for metric_name in df_metric_keys:\n",
    "        (src_name, tgt_name) = task_item\n",
    "        df_task_inputdir = inputdir+src_name+'_'+tgt_name+'/repetitive_results/df_performance_table_agg_rep_{}.csv'.format(metric_name.split('_')[1])\n",
    "        if os.path.isfile(df_task_inputdir):\n",
    "            df_task = pd.read_csv(df_task_inputdir, index_col=0)[['rep_avg']].copy()\n",
    "        else:\n",
    "            continue\n",
    "        df_task.rename(columns={'rep_avg':src_name+'_'+tgt_name}, inplace=True)\n",
    "        \n",
    "        improve_DANN = float(df_task.loc['DANN'].values[0].split('±')[0])-float(df_task.loc['source'].values[0].split('±')[0])\n",
    "        improve_target = float(df_task.loc['target'].values[0].split('±')[0])-float(df_task.loc['source'].values[0].split('±')[0])\n",
    "        df_task.loc['DANN'] = '{}({:+.3f})'.format(df_task.loc['DANN'].values[0], improve_DANN)\n",
    "        df_task.loc['target'] = '{}({:+.3f})'.format(df_task.loc['target'].values[0], improve_target)\n",
    "        \n",
    "        if 'PAD_source' in df_task.index:\n",
    "            improve_PAD_source = float(df_task.loc['PAD_DANN'].values[0].split('±')[0])-float(df_task.loc['PAD_source'].values[0].split('±')[0])\n",
    "            df_task.loc['PAD_DANN'] = '{}({:+.3f})'.format(df_task.loc['PAD_DANN'].values[0], improve_PAD_source)\n",
    "        df_task_list[metric_name].append(df_task)\n",
    "    \n",
    "    \n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(outputdir+'{}_allmetrics.xlsx'.format(output_folder.split('/')[-1]), engine='xlsxwriter')\n",
    "\n",
    "for metric_name in df_metric_keys:\n",
    "    print(metric_name)\n",
    "    if not os.path.exists(outputdir):\n",
    "        os.makedirs(outputdir)\n",
    "    print('will export data to', outputdir)\n",
    "    \n",
    "    df_task_list[metric_name] = pd.concat(df_task_list[metric_name], axis=1)\n",
    "\n",
    "    df_task_copy = df_task_list[metric_name].copy()\n",
    "    \n",
    "    plot_task_metric(df_temp, metric_name, outputdir):\n",
    "\n",
    "    perf_source = df_task_copy.loc['source'].apply(get_mean).mean()\n",
    "    perf_DANN = df_task_copy.loc['DANN'].apply(get_mean).mean()\n",
    "    perf_target = df_task_copy.loc['target'].apply(get_mean).mean()\n",
    "    df_task_list[metric_name].at['source', 'average'] = perf_source\n",
    "    df_task_list[metric_name].at['target', 'average'] = df_task_copy.loc['target'].apply(get_mean).mean()\n",
    "    df_task_list[metric_name].at['domain', 'average'] = df_task_copy.loc['domain'].apply(get_mean).mean()\n",
    "    df_task_list[metric_name]['average'] = df_task_list[metric_name]['average'].astype(object)\n",
    "    df_task_list[metric_name].at['DANN', 'average'] =  '{:.3f}({:+.3f})'.format(perf_DANN,perf_DANN-perf_source)\n",
    "    df_task_list[metric_name].at['target', 'average'] =  '{:.3f}({:+.3f})'.format(perf_target,perf_target-perf_source)\n",
    "\n",
    "    if 'PAD_source' in df_task_copy.index:\n",
    "        PAD_source = df_task_copy.loc['PAD_source'].apply(get_mean).mean()\n",
    "        PAD_DANN = df_task_copy.loc['PAD_DANN'].apply(get_mean).mean()\n",
    "        df_task_list[metric_name].at['PAD_source', 'average'] = PAD_source\n",
    "        df_task_list[metric_name].at['PAD_DANN', 'average'] = '{:.3f}({:.3f})'.format(PAD_DANN,PAD_DANN-PAD_source)\n",
    "        df_task_list[metric_name] = df_task_list[metric_name].reindex(['channel_n','batch_size','learning_rate','time_elapsed','num_params', \\\n",
    "                      'source','DANN','target','domain','PAD_source','PAD_DANN'])  \n",
    "    else:\n",
    "        df_task_list[metric_name] = df_task_list[metric_name].reindex(['channel_n','batch_size','learning_rate','time_elapsed','num_params', \\\n",
    "                  'source','DANN','target','domain'])  \n",
    "    \n",
    "    df_task_list[metric_name].loc['time_elapsed'] = df_task_list[metric_name].loc['time_elapsed'].astype(float)\n",
    "    display(df_task_list[metric_name])\n",
    "    df_task_list[metric_name].to_csv(outputdir+'df_performance_table_agg_{}.csv'.format(metric_name.split('_')[1]), encoding='utf-8')\n",
    "    df_task_list[metric_name].to_excel(writer, sheet_name=metric_name.split('_')[1])\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpLLbTjGTbZ6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U8wu607JSd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGazHVhBVcgi"
   },
   "outputs": [],
   "source": [
    "# from matplotlib import animation, rc\n",
    "\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# def makeAnimation(raw_data, labels, offset = 0, linesN=3, interval_ms=5, frames_N=100):\n",
    "#     \"\"\"makeAnimation makes animation of data and change the background color of the plot based on the labels.\n",
    "\n",
    "#     Returns anim, you can plot it in ipython notebook using the following two methods:\n",
    "#     1.  rc('animation', html='html5')\n",
    "#         anim\n",
    "#     2.  HTML(anim.to_html5_video())\n",
    "\n",
    "#     Args:\n",
    "#         raw_data (numpy array): data vector of accel and HR.\n",
    "#             raw_data.shape is (number of window, length of each window, number of channels)\n",
    "#             raw_data[:,:,0] is AccelX (red)\n",
    "#             raw_data[:,:,1] is AccelY = blue\n",
    "#             raw_data[:,:,2] is AccelZ = green\n",
    "#             raw_data[:,:,3] is HR = black\n",
    "            \n",
    "#         labels (numpy array): time vector of acc and HR.\n",
    "#         linesN (scalar): how many lines to display (max=4).\n",
    "#         interval_ms (scalar): how fast you want to display each frame.\n",
    "#         frames_N (scalar): how many windows you want to display.\n",
    "\n",
    "#     Returns:\n",
    "#         anim: a matplotlib animation object, can be used to display animation on jupyter notebook.\n",
    "    \n",
    "#     Example:\n",
    "#         anim = makeAnimation(raw_data, labels, linesN=4, interval_ms=20, frames_N=1000)\n",
    "#         rc('animation', html='html5')\n",
    "#         anim\n",
    "        \n",
    "#     Source:\n",
    "#         http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
    "\n",
    "#     \"\"\"\n",
    "#     # First set up the figure, the axis, and the plot element we want to animate\n",
    "#     fig, ax = plt.subplots()\n",
    "\n",
    "#     ax.set_xlim(( 0, 1))\n",
    "#     # ax.set_ylim((-2, 2))\n",
    "#     ax.set_ylim((data.min(), data.max()))\n",
    "    \n",
    "#     if linesN == 4:\n",
    "#         axhr_raw = ax.twinx()\n",
    "#         axhr_raw.set_xlim(( 0, 1))\n",
    "#         axhr_raw.set_ylim((40, 200))\n",
    "\n",
    "#     plotlays, plotcols = [linesN], ['red','blue','green','black']\n",
    "#     lines = []\n",
    "#     for index in range(linesN):\n",
    "#         if index == 3:\n",
    "#             lobj = axhr_raw.plot([],[],lw=2,color=plotcols[index])[0]\n",
    "#         else:\n",
    "#             lobj = ax.plot([],[],lw=2,color=plotcols[index])[0]\n",
    "#         lines.append(lobj)\n",
    "\n",
    "#     # initialization function: plot the background of each frame\n",
    "#     def init():\n",
    "#         for line in lines:\n",
    "#             line.set_data([],[])\n",
    "#         return lines    \n",
    "\n",
    "#     # animation function. This is called sequentially\n",
    "#     def animate(i):\n",
    "#         x = np.linspace(0, 1, raw_data.shape[1])\n",
    "\n",
    "#         for lnum,line in enumerate(lines):\n",
    "#             line.set_data(x, raw_data[i+offset,:,lnum]) # set data for each line separately. \n",
    "#         if labels[i+offset]:\n",
    "#             ax.set_facecolor((1.0, 0.47, 0.42))\n",
    "#         else:\n",
    "#             ax.set_facecolor('white')\n",
    "#         return lines\n",
    "    \n",
    "#     # call the animator. blit=True means only re-draw the parts that have changed.\n",
    "#     anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "#                                    frames=frames_N, interval=interval_ms, blit=True, repeat=False)\n",
    "#     return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9043,
     "status": "ok",
     "timestamp": 1583160936030,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "T-tecuc6VeSO",
    "outputId": "cb7a97f6-e894-4a5f-f6bf-46665460f9a2"
   },
   "outputs": [],
   "source": [
    "# data = src_data.data.detach().cpu().numpy().transpose(0,2,1)\n",
    "# labels = src_labels.data.detach().cpu().numpy()\n",
    "\n",
    "# anim = makeAnimation(data, labels, offset=0, linesN=3, interval_ms=200, frames_N=117)\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1582718354886,
     "user": {
      "displayName": "MICHAEL CHAN",
      "photoUrl": "",
      "userId": "10621351606155040584"
     },
     "user_tz": -480
    },
    "id": "HV0r2Gh0Vykf",
    "outputId": "cfb26497-8ab0-4b67-ba06-19b92c9bab5b"
   },
   "outputs": [],
   "source": [
    "# data.shape, labels.shape\n",
    "# # data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnK88jJ3kXd3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTuPZryA0XDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BIaUPJ-0XBz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWcFYNkc1S12"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOgrts3TNOi6xV9nChueHe9",
   "collapsed_sections": [
    "bQTp--k3JmHs",
    "fomnNHfFG02o",
    "bfJv1bL3G38c"
   ],
   "name": "stage3_model_eval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (FD_DAT)",
   "language": "python",
   "name": "fd_dat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
